"use strict";(self.webpackChunkdanielrive_site=self.webpackChunkdanielrive_site||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"eks-fluentbit","metadata":{"permalink":"/blog/eks-fluentbit","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-10-11-Configuring-Logging-in-AWS-EKS-Using-Fluent-Bit-and-CloudWatch.md","source":"@site/blog/2024-10-11-Configuring-Logging-in-AWS-EKS-Using-Fluent-Bit-and-CloudWatch.md","title":"Configuring Logging in AWS EKS Using Fluent Bit and CloudWatch","description":"fluent-bit","date":"2024-10-11T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Content related with AWS"},{"inline":true,"label":"SmartCash-Project","permalink":"/blog/tags/smart-cash-project"},{"inline":false,"label":"Kubernetes","permalink":"/blog/tags/kubernetes","description":"Content related with Kubernetes"}],"readingTime":7.96,"hasTruncateMarker":true,"authors":[{"name":"Daniel German Rivera","title":"Cloud Engineer","url":"https://github.com/danielrive","page":{"permalink":"/blog/authors/danielrivera"},"socials":{"github":"https://github.com/danielrive","linkedin":"https://www.linkedin.com/in/danielrive/"},"key":"danielrivera"}],"frontMatter":{"slug":"eks-fluentbit","title":"Configuring Logging in AWS EKS Using Fluent Bit and CloudWatch","authors":["danielrivera"],"tags":["aws","SmartCash-Project","kubernetes"]},"unlisted":false,"nextItem":{"title":"Using Terraform to push files to Git Repo for GitOps","permalink":"/blog/terraform-gitops"}},"content":"![fluent-bit](./../static/img/fluent-bit-logo.png)\\n\\nObservability is essential for any application, and the Smart-cash project is no exception. Previously [Prometheus](https://dev.to/aws-builders/adding-monitoring-to-eks-using-prometheus-operator-3ke1) was integrated for monitoring.\\n\\n\x3c!-- truncate --\x3e\\n\\nThis article is part of a personal project called Smart-cash. Previous posts covered the deployment of [AWS and Kubernetes resources](https://dev.to/aws-builders/smartcash-project-infrastructure-terraform-and-github-actions-2bo3) and [how to install FluxCD](https://dev.to/aws-builders/smartcash-project-gitops-with-fluxcd-3aep) to implement GitOps practices.\\n\\n## Project Source Code\\n\\nThe full project code can be found [here](https://github.com/danielrive/smart-cash/tree/develop), the project is still under development but you can find the terraform code to create AWS resources and also the Kubernetes manifest.\\n\\nIn this [link](https://github.com/danielrive/blog-posts/tree/main/Logging-EKS-FluentBit-CloudWatch) you can find the files used in this post\\n\\n## Option 1. Export logs directly to Cloudwatch Logs(No Cloudwatch add-on)\\n\\nThe simplest configuration involves using Fluent-Bit\'s Tail Input, which reads the logs in the host **_/var/log/containers/*.log_** and sends them to Cloudwatch. This approach could be enough if you want to centralize the logs in CloudWatch or maybe another platform.\\n\\n### Fluent-Bit installation\\n\\nThe Fluent-Bit Helm chart will be used in combination with FluxCD.\\n\\nAdding the FluxCD source:\\n\\n```yaml\\nkind: HelmRepository\\nmetadata:\\n  name: fluent-bit\\n  namespace: flux-system\\nspec:\\n  interval: 10m0s\\n  url: https://fluent.github.io/helm-charts\\n```\\n\\n#### Adding the Helm chart\\n\\nThe supported values for the Fluent-Bit Helm chart can be found [here](https://github.com/fluent/helm-charts/blob/main/charts/fluent-bit/values.yaml).\\n\\nTo use Fluent Bit with AWS, the following requirements must be met:\\n\\n- **IAM Roles for Service Accounts (IRSA)**: You must set up an IAM role with permissions to create CloudWatch Logs streams and write logs. This role should be associated with the service account that Fluent Bit uses. AWS EKS Pod identity is also an option.\\n- **CloudWatch Log Group**: You can either create the CloudWatch Log group in advance or allow Fluent Bit to handle the log group creation.\\n\\nThe main configuration is shown below (some lines have been omitted for brevity). The full configuration file can be found [link](https://github.com/danielrive/blog-posts/tree/main/Logging-EKS-FluentBit-CloudWatch)\\n\\nFluent-Bit will run as a DaemonSet. The Helm chart will create the RBAC and the Service account, named fluent-bit, which will be annotated with the AWS IAM role with the appropriate CloudWatch permissions.\\n\\n```yaml\\napiVersion: helm.toolkit.fluxcd.io/v2beta1\\nkind: HelmRelease\\n###ommited-lines\\nspec:\\n  chart:\\n    spec:\\n      chart: fluent-bit\\n      version: 0.47.9\\n      ###ommited-lines\\n  values:\\n    kind: DaemonSet\\n    ###ommited-lines\\n    serviceAccount:\\n      create: true\\n      annotations: \\n        eks.amazonaws.com/role-arn: arn:aws:iam::${ACCOUNT_NUMBER}:role/role-fluent-bit-${ENVIRONMENT}\\n    rbac:\\n      create: true\\n    ###ommited-lines\\n```\\n\\nA volume is needed for Filesystem buffering, which helps to manage [backpressure and overall memory control](https://docs.fluentbit.io/manual/administration/buffering-and-storage#filesystem-buffering-to-the-rescue), also this is used to store the position (or offsets) of the log files being read, allowing Fluent Bit to track its progress and resume from the correct position if needed.\\n\\n```yaml\\nextraVolumes:\\n      - name: fluentbit-status\\n        hostPath:\\n          path: /var/fluent-bit/state\\n    extraVolumeMounts:\\n      - name: fluentbit-status\\n        mountPath: /var/fluent-bit/state\\n```\\n\\nConfigs section defines the [Inputs](https://docs.fluentbit.io/manual/pipeline/inputs), [Filters](https://docs.fluentbit.io/manual/pipeline/filters), and [Outputs](https://docs.fluentbit.io/manual/pipeline/outputs) for collecting and processing data. For this scenario an Input of Tail type is configured to read the content of the files located at **_/var/log/containers/*.log_**. Let\'s break down the configuration details:\\n\\n**Note:** AWS has some advanced configurations for inputs, filters and output, you can check this [link](https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/main/k8s-quickstart/cwagent-operator-rendered.yaml)\\n\\nThe service section defines the global properties for Fluent-Bit, for this case:\\n\\n- **Flush Interval**: Set to 1 second, meaning Fluent Bit will send the collected logs to the configured output destinations every second.\\n\\n- **Log Level**: Set to Info, which includes informational messages as well as warnings and errors.\\n\\n- The storage path for Filesystem buffering is the volume mounted in previous configurations with a backlog memory limit of 5M, which means that if Fluent-bit service reaches this limit, it stops loading any more backlog chunks from the storage path into memory.\\n\\n```yaml\\nconfig:\\n  service: |\\n    [SERVICE]\\n      Daemon Off\\n      Flush  1\\n      Log_Level  info\\n      Parsers_File /fluent-bit/etc/parsers.conf\\n      Parsers_File /fluent-bit/etc/conf/custom_parsers.conf\\n      HTTP_Server On\\n      HTTP_Listen 0.0.0.0\\n      HTTP_Port  \\\\{\\\\{ .Values.metricsPort \\\\}\\\\}\\n      Health_Check On\\n      storage.path  /var/fluent-bit/state/flb-storage/\\n      storage.sync              normal\\n      storage.checksum          off\\n      storage.backlog.mem_limit 5M\\n```\\n\\nInputs define the data sources that Fluent Bit will collect logs from. In this scenario, the Tail input is used, which allows Fluent Bit to monitor one or more text files. Key points for this configuration include:\\n\\n- In common Kubernetes environments, container runtimes store logs in the **_/var/log/pod/_** and **_/var/log/containers/_** (containers directory has symlinks to pod directory). Each log file follows a naming convention that includes key information like the pod name, namespace, container name, and container ID, for this case entries for fluent-bit, cloudwath-agent,kube-proxy, and aws-node will be ignored\\n- Some log entries may span multiple lines. Fluent Bit handles multi-line logs with built-in modes, and for this scenario, the Docker or CRI modes are used to process them correctly.\\n- To track the last line read from each log file, Fluent Bit uses a database to store this position. The database is saved in the previously mounted volume, ensuring that Fluent Bit can resume reading from the correct location.\\n\\n```yaml\\n[INPUT]\\n   Name                tail\\n   Tag                 applications.*\\n   Exclude_Path        /var/log/containers/cloudwatch-agent*, /var/log/containers/fluent-bit*, /var/log/containers/aws-node*, /var/log/containers/kube-proxy*\\n   Path                /var/log/containers/*.log\\n   multiline.parser    docker, cri\\n   DB                  /var/fluent-bit/state/flb_container.db\\n   Mem_Buf_Limit       50MB\\n   Skip_Long_Lines     On\\n   Refresh_Interval    10\\n   storage.type        filesystem\\n   Rotate_Wait         30\\n```\\n\\nOutputs define where the collected data is sent, and Fluent-Bit provides a plugin to send logs to CloudWatch.\\n\\n- If you check the Input configurations there is a tag defined, **_applications.*_**. this helps to assign a label to the logs collected for that Input, in this case, it ensures that logs with this tag are routed to the specified output destination.\\n- CloudWatch log groups can be created by Fluent Bit, but in this scenario, the creation is disabled (set to off) since Terraform is used to manage log groups.\\n- The **_log_stream_prefix_** sets a prefix for the log streams created in CloudWatch, helping organize and identify the log entries within the stream.\\n\\n```yaml\\n[OUTPUT]\\n   Name cloudwatch_logs\\n   Match applications.*\\n   region ${AWS_REGION} \\n   log_group_name /aws/eks/${CLUSTER_NAME}/workloads\\n   log_stream_prefix from-k8-fluent-bit-\\n   auto_create_group off\\n```\\n\\nOnce you deploy the Helm chart you can check the CloudWatch service, if everything is working you should see some stream created, in this case out prefix is from-k8-fluent-bit-\\n\\n![Log-Stream](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mtyh8dejch7iqqsbbnj5.png)\\n\\nand the log entry\\n\\n![log-entry](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/azg2f14qnolys0nfb687.png)\\n\\n#### Adding a filter\\n\\nFilters in Fluent Bit allow you to enrich the data being collected. For instance, the Kubernetes Filter adds valuable metadata to log entries, such as namespace, pod_name, host, and more.\\n\\nHere are some key points about the filter configuration:\\n\\n- The Tag from the input configuration is reused here to extract information like pod_name, namespace, and other relevant metadata.\\n\\n- The Kube_URL points to the Kubernetes API server, which Fluent Bit queries to obtain metadata about the pods involved in the logs. The path for the token and certificate is specified in Kube_CA_File and Kube_Token_File.\\n\\n- You can configure the filter to include annotations and labels from the pods in the log entries.\\n\\n**Note:** Be cautious about Fluent Bit querying the API server for metadata. In clusters with a high number of resources, [this can put an additional](https://aws.amazon.com/blogs/containers/capturing-logs-at-scale-with-fluent-bit-and-amazon-eks/) load on the API server. [One optimization](https://aws.amazon.com/blogs/containers/capturing-logs-at-scale-with-fluent-bit-and-amazon-eks/) is to retrieve pod metadata from the node\u2019s kubelet instead of the kube-apiserver, but this requires enabling hostNetwork in the DaemonSet\\n\\n```yaml\\n[FILTER]\\n   Name   kubernetes\\n   Match  applications.*\\n   Kube_URL      https://kubernetes.default.svc:443\\n   Kube_CA_File       /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\n   Kube_Token_File \\n     /var/run/secrets/kubernetes.io/serviceaccount/token\\n   Kube_Tag_Prefix     application.var.log.containers.\\n   Merge_Log           On\\n   Merge_Log_Key       log_processed\\n   K8S-Logging.Parser  On\\n   K8S-Logging.Exclude Off\\n   Labels              On\\n   Annotations         Off\\n   Buffer_Size         0\\n```\\n\\nAfter applying this filter the logs should have pods metadata\\n\\n![after-filter](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3pb61s2hu5gbth72m4xy.png)\\n\\n## Option 2. Use the amazon-cloudwatch-observability add-on\\n\\nContainer Insights can be used to collect, aggregate, and summarize both metrics and logs. If you plan to enable this in your EKS cluster, the Amazon CloudWatch Observability add-on installs the necessary resources to achieve this.\\n\\nAt a high level, the add-on installs two key components:\\n\\n- A CloudWatch agent to collect metrics.\\n- Fluent-Bit to collect logs, using the [AWS for Fluent-bit container image](https://github.com/aws/aws-for-fluent-bit).\\n\\nBoth components are deployed as DaemonSets.\\n\\nThe add-on can be installed via Terraform or by using a [Helm-chart](https://github.com/aws-observability/helm-charts/blob/main/charts/amazon-cloudwatch-observability/values.yaml), Regardless of the method, you\'ll need to create an IAM role for the service account **_cloudwatch-agent_** in the **_amazon-cloudwatch_** namespace.\\n\\n```hcl\\nresource \\"aws_eks_addon\\" \\"cloudwatch\\" {\\n  cluster_name                = aws_eks_cluster.kube_cluster.name\\n  addon_name                  = \\"amazon-cloudwatch-observability\\"\\n  addon_version               = \\"\\"v2.1.2-eksbuild.1\\"\\"\\n  service_account_role_arn    = aws_iam_role.cloudwatch_role.arn \\n  resolve_conflicts_on_update = \\"OVERWRITE\\"\\n}\\n```\\n\\nThe add-on creates several resources in the cluster, some of which you may not need. For example, if you list the DaemonSets in the amazon-cloudwatch namespace, you\'ll notice seven DaemonSets, some of which might have 0 replicas. While these resources may not be actively used, they still exist in your cluster and can create some noise.\\n\\n![addon-installed](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1oxdmpgpr12lfg4q4xk8.png)\\n\\nYou can customize the add-on configurations to suit your needs. For example, you can disable Fluent Bit logs for Accelerated Compute monitoring or skip collecting NVIDIA GPU metrics.\\n\\n```hcl\\nresource \\"aws_eks_addon\\" \\"cloudwatch\\" {\\n  cluster_name                = aws_eks_cluster.kube_cluster.name\\n  addon_name                  = \\"amazon-cloudwatch-observability\\"\\n  addon_version               = \\"\\"v2.1.2-eksbuild.1\\"\\"\\n  service_account_role_arn    = aws_iam_role.cloudwatch_role.arn \\n  resolve_conflicts_on_update = \\"OVERWRITE\\"\\n  configuration_values = jsonencode({\\n    containerLogs = {\\n        enabled = true\\n    },    \\n    agent = {\\n      config = {\\n        logs = {\\n          metrics_collected = {\\n            application_signals = {},\\n            kubernetes = {\\n              \\"enhanced_container_insights\\": true\\n              \\"accelerated_compute_metrics\\": false\\n            }}}}}\\n  })\\n}\\n```\\n\\nBy default, the add-on creates four CloudWatch log groups. The following image, taken from the [AWS documentation](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-logs-FluentBit.html#Container-Insights-FluentBit-setup), explains the naming structure of the log groups and the type of data each group stores.\\n\\n![AWS-Logs-group](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/yzv79wm5xrnu9rozjf96.png)\\n\\nTo change expiration days and names for the groups is better to use the Helm chart instead of the Terraform code to install the add-on. You can do this by modifying the fluent-bit outputs.\\n\\nThe last log group is named performance and stores metrics collected by the CloudWatch agent, such as the number of running pods, CPU usage, and memory metrics.\\n\\n### Bonus: Cluster dashboard\\n\\nAs mentioned earlier, the CloudWatch add-on collects, aggregates, and summarizes metrics. Once the add-on is installed, AWS automatically generates a dashboard that provides useful insights and metrics for your cluster.\\n\\n![Cluster-dashboard](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/z8nhm4tzefmu4w1shgvr.png)\\n\\nYou can watch metric per pod\\n\\n![metric-per-pod](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ttwtfahdt2htcy5yh4ok.png)\\n\\nIt also generates a visual map that organizes Kubernetes resources by namespace.\\n\\n![Cluster-resource-mapping](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ro4q55u98jm2pbkk8r7u.png)"},{"id":"terraform-gitops","metadata":{"permalink":"/blog/terraform-gitops","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-07-11-Using-Terraform-to-push-files-to-Git-Repo-for-GitOps.md","source":"@site/blog/2024-07-11-Using-Terraform-to-push-files-to-Git-Repo-for-GitOps.md","title":"Using Terraform to push files to Git Repo for GitOps","description":"In this article, I will share my thoughts about using Terraform in the GitOps process, specifically to create the manifest and push it to the Git repo.","date":"2024-07-11T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Content related with AWS"},{"inline":true,"label":"SmartCash-Project","permalink":"/blog/tags/smart-cash-project"},{"inline":false,"label":"Kubernetes","permalink":"/blog/tags/kubernetes","description":"Content related with Kubernetes"}],"readingTime":2.935,"hasTruncateMarker":true,"authors":[{"name":"Daniel German Rivera","title":"Cloud Engineer","url":"https://github.com/danielrive","page":{"permalink":"/blog/authors/danielrivera"},"socials":{"github":"https://github.com/danielrive","linkedin":"https://www.linkedin.com/in/danielrive/"},"key":"danielrivera"}],"frontMatter":{"slug":"terraform-gitops","title":"Using Terraform to push files to Git Repo for GitOps","authors":["danielrivera"],"tags":["aws","SmartCash-Project","kubernetes"]},"unlisted":false,"prevItem":{"title":"Configuring Logging in AWS EKS Using Fluent Bit and CloudWatch","permalink":"/blog/eks-fluentbit"},"nextItem":{"title":"Adding monitoring to EKS using Prometheus operator","permalink":"/blog/EKS-Prometheus"}},"content":"In this article, I will share my thoughts about using Terraform in the GitOps process, specifically to create the manifest and push it to the Git repo.\\n\\n![simple image](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/chouc9wyoln2u16pvejf.png)\\n\\n\x3c!-- truncate --\x3e\\n\\n## The basics\\n\\nGitOps relies on a Git repository as the single source of truth. New commits imply infrastructure and application updates.\\n\\nImagine a Git repository where you push all the manifests of the Kubernetes resources you want to create in your cluster. These are pulled by a tool or script that runs a \\"kubectl apply\\", creates the resources, and checks the Git repo for new changes to apply. This, at a high level, is GitOps.\\n\\n## Setting up the scenario\\n\\nFor this case, the K8 cluster will run in AWS EKS, and Terraform is being used as an IaC tool.\\n\\nA basic cluster can be created using Terraform. You can check an example [here](https://github.com/danielrive/smart-cash/blob/main/infra/terraform/modules/eks/main.tf).\\n\\nFluxCD installation can be done using [the official documentation](https://fluxcd.io/flux/installation/bootstrap/github/) or you can check [this](https://dev.to/aws-builders/smartcash-project-gitops-with-fluxcd-3aep).\\n\\nI will not explain some Flux concepts like sources and Kustomizations; you can check that in the links shared previously.\\n\\n## Creating the YAML files\\n\\nLet\'s say that we want to create a namespace for the development environment, we can use the following YAML:\\n\\n```YAML\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: develop\\n  labels:\\n    test: true\\n```\\n\\nWe can push this file to GitHub and wait for FluxCD to do the magic.\\n\\nNow let\'s say that we want to create a service account and associate it with an AWS IAM role, the YAML can be:\\n\\n```YAML\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: sa-test-develop\\n  annotations:  \\n    eks.amazonaws.com/role-arn: arn:aws:iam::12345678910:role/TEST\\n```\\n\\nThis looks easy but what happens if we have multiple environments or if We don\'t yet know the ARN of the role because this is part of our IaC?\\n\\n![help-me](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1l8cmmw835be29z1fak2.png)\\n\\nHere is where Terraform gives us a hand.\\n\\nYou can create something like a template for the manifest and some variables that you can specify with Terraform. The two manifests would look like:\\n\\n```YAML\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: ENVIRONMENT\\n  labels:\\n    test: true\\n```\\n\\n```YAML\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: sa-test-ENVIRONMENT\\n  annotations:  \\n    eks.amazonaws.com/role-arn: ROLE_ARN\\n```\\n\\nNotice the ENVIRONMENT and ROLE_ARN variables added.\\n\\nWe can use the [Terraform GitHub provider](https://registry.terraform.io/providers/integrations/github/latest/docs) to push the file to the repository. Let\'s check the following code to push the service account:\\n\\n``` terraform\\nresource \\"github_repository_file\\" \\"sa-test\\" {\\n  repository          = data.github_repository.flux-gitops.name\\n  branch              = main\\n  file                = \\"./manifest/sa-manifest.yaml\\"\\n  content = templatefile(\\n    \\"sa-manifest.yaml\\",\\n    {\\n      ENVIRONMENT = var.environment\\n      ROLE_ARN = aws_iam_role.arn\\n    }\\n  )\\n  commit_message      = \\"Terraform\\"\\n  commit_author       = \\"terraform\\"\\n  commit_email        = \\"example@example\\"\\n  overwrite_on_create = true\\n}\\n```\\n\\nThe arguments **repository** and **branch** allow us to specify the remote repo and the branch where we want to push the file. The **file** argument is the location **in the remote repository** where we want to put the file.\\n\\nThe **content** argument is where we pass the values to the variables created in the template, in this case ENVIRONMENT and ROLE_ARN, the values are a terraform variable and the reference to a Terraform resource that creates the role.\\n\\n**overwrite_on_create** argument is needed because if you run Terraform again, it will show an error because the file already exists in the repo.\\n\\n## Pros\\n\\n1. Pushing the manifests using Terraform avoids the manual tasks of committing and pushing them, allowing us to automate more steps.\\n2. We can integrate this process into our pipeline, so a full environment can be ready when the pipeline finishes.\\n3. Terraform count can be used when there are many manifests to push, avoiding repetitive code."},{"id":"EKS-Prometheus","metadata":{"permalink":"/blog/EKS-Prometheus","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-11-30-Adding-monitoring-to-EKS-using-Prometheus-operator.md","source":"@site/blog/2023-11-30-Adding-monitoring-to-EKS-using-Prometheus-operator.md","title":"Adding monitoring to EKS using Prometheus operator","description":"eks+prometheus+grafana","date":"2023-11-30T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Content related with AWS"},{"inline":true,"label":"SmartCash-Project","permalink":"/blog/tags/smart-cash-project"},{"inline":false,"label":"Kubernetes","permalink":"/blog/tags/kubernetes","description":"Content related with Kubernetes"}],"readingTime":5.375,"hasTruncateMarker":true,"authors":[{"name":"Daniel German Rivera","title":"Cloud Engineer","url":"https://github.com/danielrive","page":{"permalink":"/blog/authors/danielrivera"},"socials":{"github":"https://github.com/danielrive","linkedin":"https://www.linkedin.com/in/danielrive/"},"key":"danielrivera"}],"frontMatter":{"slug":"EKS-Prometheus","title":"Adding monitoring to EKS using Prometheus operator","authors":["danielrivera"],"tags":["aws","SmartCash-Project","kubernetes"]},"unlisted":false,"prevItem":{"title":"Using Terraform to push files to Git Repo for GitOps","permalink":"/blog/terraform-gitops"},"nextItem":{"title":"GitOps with FluxCD","permalink":"/blog/fluxcd"}},"content":"![eks+prometheus+grafana](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/404iotfmmlhlrsm8swt7.png)\\n\\nPrevious articles showed how to [build the EKS Infrastructure in AWS](https://dev.to/aws-builders/smartcash-project-infrastructure-terraform-and-github-actions-2bo3) and [how to install FluxCD](https://dev.to/aws-builders/smartcash-project-gitops-with-fluxcd-3aep) to implement GitOps practices, This article is focused on explaining the steps taken to install the Prometheus Operator(using Helm) and Grafana for monitoring.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Source Code\\n\\nThe source code for this project can be found [here](https://github.com/danielrive/smart-cash/releases/tag/v1.3.0), also a [GitOps repository](https://github.com/danielrive/smart-cash-gitops-flux) has been created to store the Yaml files that FluxCD uses and apply to the EKS cluster.\\n\\n## Prometheus operator\\n\\n> The Prometheus Operator provides Kubernetes native deployment and management of Prometheus and related monitoring components.\\n\\nThe [Prometheus operator](https://prometheus-operator.dev/docs/operator/design/) defines Kubernetes Custom Resources and controllers that facilitate installing Prometheus. The community has developed alternative options such as _kube-Prometheus_ and _kube-prometheus-stack_ to install the components to monitor Kubernetes.\\n\\n### Prometheus Operator, kube-prometheus and kube-prometheus-stack\\n\\nThe project repository for Prometheus-operator can be found [here](https://github.com/prometheus-operator/prometheus-operator), The repo defines the CRDs and the controller. You can follow this [documentation](https://prometheus-operator.dev/docs/user-guides/getting-started/) for the installation. which will require the creation of metrics exporters, node exporters, scrape configurations, etc.\\n\\nOn the other hand, the [Kube-prometheus](https://github.com/prometheus-operator/kube-prometheus) project provides documentation and scripts to operate end-to-end Kubernetes cluster monitoring using the Prometheus Operator, making easier the process of monitoring the Kubernetes cluster.\\n\\n[kube-prometheus-stack](https://github.com/prometheus-community/helm-charts) is a Helm chart that contains several components to monitor the Kubernetes cluster, along with Grafana dashboards to visualize the data. This option will be used in this article.\\n\\n## Installing kube-prometheus-stack Helm chart\\n\\nIn previous articles, FluxCD was installed in EKS cluster to implement GitOps, the following flux source will now be added.\\n\\n``` YAML\\napiVersion: source.toolkit.fluxcd.io/v1beta2\\nkind: HelmRepository\\nmetadata:\\n  name: helm-repo-prometheus\\n  namespace: flux-system\\nspec:\\n  interval: 10m0s\\n  url: https://prometheus-community.github.io/helm-charts\\n```\\n\\nAlso, a Flux Helm release is added\\n\\n``` YAML\\napiVersion: helm.toolkit.fluxcd.io/v2beta1\\nkind: HelmRelease\\nmetadata:\\n  name: prometheus\\n  namespace: monitoring\\nspec:\\n  interval: 10m0s\\n  chart:\\n    spec:\\n      chart: kube-prometheus-stack\\n      sourceRef:\\n        kind: HelmRepository\\n        name: helm-repo-prometheus\\n        namespace: flux-system\\n  values:\\n    defaultRules:\\n      rules:\\n        etcd: false\\n        kubeSchedulerAlerting: false\\n        kubeSchedulerRecording: false\\n        windows: false\\n    prometheus:\\n      prometheusSpec:\\n        storageSpec:\\n            volumeClaimTemplate:\\n              spec:\\n                storageClassName: aws-ebs-gp2\\n                accessModes: [\\"ReadWriteOnce\\"]\\n                resources:\\n                  requests:\\n                    storage: 40Gi\\n```\\n\\nIf you examine the values for the chart, by default, it installs rules to monitor etcd and some other control plane components. However, in this case, that is not necessary due to EKS limiting access to certain control-plane components.\\n\\nBy default, Prometheus uses local storage to store data. To enable persistent storage, an EBS volume can be added. In this scenario, the [EBS CSI](https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html) driver is employed, and a storage class is defined to manage the integration with Prometheus.\\n\\nOnce the manifests are ready, you can push them to the GitOps repo( [here](https://github.com/danielrive/smart-cash-gitops-flux/blob/main/common/helm-prometheus.yaml) for this case), and wait for Flux to handle the installation process in the cluster.\\n\\nYou can check the cluster by looking for the resources created, notice that for this case, everything will be placed in the **monitoring** namespace.\\n\\n`kubectl get crd`\\n\\n![prometheus-cdr](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qfjv7qgaitj5a0pmmsfc.png)\\n\\nAdditionally, certain deployments and services should have been created in the monitoring namespace.\\n\\n`kubectl get pods -n monitoring`\\n\\n![pods](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6suj0e8ez0kf3jkn3vde.png)\\n\\nLet\'s look at the Prometheus server console, you can expose the service by an ingress or using port-forward.\\n\\n`kubectl port-forward service/prometheus-kube-prometheus-prometheus 3001:9090 -n monitoring`\\n\\nThe previous command will expose the Prometheus service in localhost:3001, you can go directly to the targets and you should see some targets created automatically, as well as the metrics and the services discovered by the server.\\n\\n![targets](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/z0a5ff0d9w9es56q4gtu.png)\\n\\nThis is useful because you don\'t need to configure the targets to monitor K8 and node metrics, the Helm chart does this for you. For instance, a simple example is just to check the number of pods created in the default namespace, you can run this PromQL query.\\n\\n`count(kube_pod_created{namespace=\\"default\\"})`\\n\\n![simple-query](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dwt7ryjpjyay53x2q9r7.png)\\n\\n### Grafana Dashboards\\n\\nHelm chart also installs Grafana and configures some useful dashboards that you can use, if you list the services and pods you will see some resources related to Grafana. You can expose the Grafana service or create an ingress for it.\\n\\n#### Creating nginx ingress for Grafana\\n\\nNginx-ingress is utilized and installed using Helm. You can add the following Flux source and the Helm release to the GitOps repo. Check the GitOps repo for this project [here](https://github.com/danielrive/smart-cash-gitops-flux/blob/main/common/helm-nginx-ingress.yaml) and use it as a model.\\n\\nHelm source\\n\\n```yaml\\napiVersion: source.toolkit.fluxcd.io/v1beta2\\nkind: HelmRepository\\nmetadata:\\n  name: helm-repo-nginx-ingress\\n  namespace: flux-system\\nspec:\\n  interval: 10m0s\\n  type: oci\\n  url: oci://ghcr.io/nginxinc/charts\\n```\\n\\nUse this yaml to install the chart, in this case AWS Network Load Balancer is used, this is done through the annotation specified in the values for the chart.\\n\\n``` Yaml\\napiVersion: helm.toolkit.fluxcd.io/v2beta1\\nkind: HelmRelease\\nmetadata:\\n  name: nginx-ingress\\n  namespace: nginx-ingress\\nspec:\\n  interval: 10m0s\\n  chart:\\n    spec:\\n      chart: nginx-ingress\\n      version: 1.0.2\\n      sourceRef:\\n        kind: HelmRepository\\n        name: helm-repo-nginx-ingress\\n        namespace: flux-system\\n  values:\\n    controller:\\n      service:\\n        annotations: \\n          service.beta.kubernetes.io/aws-load-balancer-type: \\"nlb\\"\\n```\\n\\n#### Installing Cert-managet to support SSL\\n\\n> This article will not dig into details about cert-manager concepts.\\n\\nIn order to support SSL in the EKS cluster cert-manager will be used, cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates.\\n\\nCert-manager uses Kubernetes CRD, to install them you can run:\\n\\n`kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.2/cert-manager.crds.yaml`\\n\\nThis also can be added to the GitOps repo(check [here](https://github.com/danielrive/smart-cash-gitops-flux/blob/main/common/crd-cert-manager.yaml)) and given to Flux to handle it.\\n\\nWhen the CRDs are ready, you can install cert-manager, in this case a Helm chart will be used, this also will be added in GitOps repo.\\n\\n```YAML\\napiVersion: helm.toolkit.fluxcd.io/v2beta1\\nkind: HelmRelease\\nmetadata:\\n  name: cert-manager\\n  namespace: cert-manager\\nspec:\\n  interval: 10m0s\\n  chart:\\n    spec:\\n      chart: cert-manager\\n      version: 1.13.2\\n      sourceRef:\\n        kind: HelmRepository\\n        name: helm-cert-manager\\n        namespace: flux-system\\n  values:\\n    serviceAccount:\\n      annotations:\\n        eks.amazonaws.com/role-arn: arn:aws:iam::123456789:role/cert-manager-us-west-2\\n    securityContext:\\n      fsGroup: 1001\\n    extraArgs:\\n      - --issuer-ambient-credentials\\n\\n```\\n\\nFinally and cert-manager ClusterIssuer is added, in this case the domain will be validated in AWS Route53, this is done through the IAM role passed in the previous YAML file for the Helm chart.\\n\\n```YAML\\napiVersion: cert-manager.io/v1\\nkind: ClusterIssuer\\nmetadata:\\n  name: example-letsencrypt2\\nspec:\\n  acme:\\n    email: notreply@example.info\\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\\n    privateKeySecretRef:\\n      name: example-issuer-account-key\\n    solvers:\\n    - selector:\\n        dnsZones:\\n          - \\"example.info\\"\\n      dns01:\\n        route53:\\n          region: us-west-2\\n```\\n\\nOnce cert-manager and nginx-ingress are installed you can create an ingress for Grafana. The following manifest has been added to the GitOps repo.\\n\\n``` YAML\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: grafana-ingress\\n  namespace: monitoring\\n  annotations:\\n    cert-manager.io/cluster-issuer: example-letsencrypt2\\nspec:\\n  ingressClassName: nginx\\n  rules:\\n  - host: monitoring.example.info\\n    http:\\n      paths:\\n      - path: /\\n        pathType: Prefix\\n        backend:\\n          service:\\n            name: prometheus-grafana\\n            port:\\n              number: 80\\n  tls:\\n   - hosts:\\n     - monitoring.example.info\\n     secretName: example-issuer-account-ingress-key2\\n```\\n\\nWith this installed you can browser and access Grafana, you should see some dashboards already created.\\n\\n![Grafana-dash](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/cxhfnhv82jfuk3b1kj8t.png)\\n\\nFor instance, the _Kubernetes/API server_ dashboard is pre-configured, also you can also use third-party dashboards.\\n\\n![Grafana-k8-api](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7zilyb7s756tkm696r4g.png)"},{"id":"fluxcd","metadata":{"permalink":"/blog/fluxcd","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-11-04-GitOps-with-FluxCD.md","source":"@site/blog/2023-11-04-GitOps-with-FluxCD.md","title":"GitOps with FluxCD","description":"GitOps meme, source https://blog.kubesimplify.com/gitops-demystified","date":"2023-11-04T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Content related with AWS"},{"inline":true,"label":"SmartCash-Project","permalink":"/blog/tags/smart-cash-project"},{"inline":false,"label":"Kubernetes","permalink":"/blog/tags/kubernetes","description":"Content related with Kubernetes"}],"readingTime":8.455,"hasTruncateMarker":true,"authors":[{"name":"Daniel German Rivera","title":"Cloud Engineer","url":"https://github.com/danielrive","page":{"permalink":"/blog/authors/danielrivera"},"socials":{"github":"https://github.com/danielrive","linkedin":"https://www.linkedin.com/in/danielrive/"},"key":"danielrivera"}],"frontMatter":{"slug":"fluxcd","title":"GitOps with FluxCD","authors":["danielrivera"],"tags":["aws","SmartCash-Project","kubernetes"]},"unlisted":false,"prevItem":{"title":"Adding monitoring to EKS using Prometheus operator","permalink":"/blog/EKS-Prometheus"},"nextItem":{"title":"AWS Infrastructure Terraform and GitHub Actions","permalink":"/blog/github-actions"}},"content":"![GitOps meme, source https://blog.kubesimplify.com/gitops-demystified](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/t18cxoi3v9353gucvs2p.png)\\n\\nIn a [previous article](https://dev.to/aws-builders/smartcash-project-infrastructure-terraform-and-github-actions-2bo3) I mentioned the idea behind this project that I named SmartCash. I began building the terraform code for the infrastructure in AWS and the pipeline to deploy it.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn this article, I will introduce FluxCD as a GitOps tool and demonstrate its usage.\\n\\n## Source code\\n\\nA new release has been created in the smart-cash repository for the project. v1.1.0 version will be used, you can check the repository [here](https://github.com/danielrive/smart-cash/tree/v1.1.0).\\n\\nAdditionally, a new repository will be created to store the K8 manifest that will be synced with the EKS cluster using FluxCD, you can view the repo [here](https://github.com/danielrive/smart-cash-gitops-flux).\\n\\n## A quick introduction to GitOps\\n\\nGitOps is an operational model for cloud-native architectures,  it relies on a Git repository as the single source of truth. New commits imply infrastructure and application updates.\\n\\nOpenGitOps group has defined 5 principles, and while I won\'t delve into them, [here](https://opengitops.dev/), you can read more. If you take a look at those principles you will see that they are, in some sense related to some Kubernetes concepts.\\n\\nA great book to gain a better understanding of GitOps history and concepts is **[The Path to GitOps](https://developers.redhat.com/e-books/path-gitops)**.\\n\\nIn summary, GitOps is centered around using a Git repository for defining and managing both infrastructure and application configurations through a Git-based workflow.\\n\\n### What is FluxCD\\n\\n[FluxCD](https://fluxcd.io/) is an open-source GitOps operator for Kubernetes, you can declaratively define the desired state of your infrastructure and configurations in a Git repository. Flux monitors the repository and applies updates to the Kubernetes cluster when new changes arrive.\\n\\nFlux started as a monolith but in v2 it was broken up into individual components called GitOps Toolkit, this refers collection of specialized tools, Flux Controllers, composable APIs, and reusable Go packages available under the fluxcd GitHub organization.\\n\\nCore concepts and toolkit components are described [here](https://www.weave.works/technologies/what-is-flux-cd/).\\n\\n![hands-on](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hv4kfhyq6yq90x3e27wh.png)\\n\\n## Installing FluxCD in the cluster\\n\\nFluxCD [installation](https://fluxcd.io/flux/installation/) can be done by Flux CLI, the most straightforward method can be done by the [**_flux bootstrap_** command](https://fluxcd.io/flux/installation/bootstrap/), this deploys the Flux controllers on the K8 cluster and configures them to synchronize the cluster to the Git repository, if the Git Repo doesn\'t exist, the bootstrap command will create it.\\n\\nTo incorporate FluxCD installation into this project a new bash script has been added into the repository that contains the terraform code, this bash script will be execute by terraform as a null resource.\\n\\n``` bash\\n#/bin/bash\\n\\n## Configure Cluster Credentials\\n\\n# $1 = CLUSTER_NAME\\n# $2 = AWS_REGION\\n# $3 = GH_USER_NAME\\n# $4 = FLUX_REPO_NAME\\n\\necho \\"----------\x3e  get eks credentials\\"\\naws eks update-kubeconfig --name $1  --region $2\\n\\n## validate if flux is installed\\n\\nflux_installed=$(kubectl api-resources | grep flux)\\nif [ -z \\"$flux_installed\\" ]; then\\n  echo \\"----------\x3e  flux is not installed\\"\\n\\n  ### install flux\\n\\n  echo \\"----------\x3e  installing flux cli\\"\\n\\n  curl -s https://fluxcd.io/install.sh | sudo bash\\n\\n  echo \\"----------\x3e  run flux bootstrap\\"\\n  flux bootstrap github \\\\\\n    --owner=$3 \\\\\\n    --repository=$4 \\\\\\n    --path=\\"clusters/$1/bootstrap\\" \\\\\\n    --branch=main \\\\\\n    --personal\\nelse\\n  echo \\"----------\x3e  flux is installed\\"\\nfi\\n```\\n\\nThe _flux bootstrap github_ command deploys the Flux controllers on the K8 cluster and configures the controllers to synchronize the Git repo with the cluster. This is done by some K8 manifests that are created and pushed to the repo in the path passed in the command.\\n\\nIt\'s worth noting that some env variables like FLUX_REPO_NAME, and GH_USER_NAME are used by the bash script, these variables are passed as an argument in the bash script execution.\\n\\n### Adding FluxCD bootstrap script to terraform code\\n\\nThe bash script will be executed in the GH workflow template created to deploy the infrastructure, the following job is added to the Workflow template.\\n\\n```terraform\\n#### bash script arguments\\n  # $1 = CLUSTER_NAME\\n  # $2 = AWS_REGION\\n  # $3 = GH_USER_NAME\\n  # $4 = FLUX_REPO_NAME\\n\\nresource \\"null_resource\\" \\"bootstrap-flux\\" {\\n  depends_on          = [module.eks_cluster]\\n  provisioner \\"local-exec\\" {\\n    command = <<EOF\\n    ./scripts/bootstrap-flux.sh ${local.cluster_name}  ${var.region} ${local.gh_username} ${data.github_repository.flux-gitops.name}\\n    EOF\\n  }\\n  triggers = {\\n    cluster_oidc = module.eks_cluster.cluster_oidc\\n    created_at   = module.eks_cluster.created_at\\n  }\\n\\n}\\n```\\n\\nNotice that the _GITHUB_TOKEN_ variable is passed directly in the Github job.\\n\\nOnce the workflow is ready you can push it to the repo and see how terraform will create all the infra and after EKS cluster creation will execute the bash script.\\n\\nYou can run **flux check** command locally to validate the status of the installation(you should have access to the cluster in your local env)\\n\\n![flux-check](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/czt08fmy5h10p2gq0na3.png)\\n\\nIf you take a look at the above image you will see that the Source Controller is deployed, [Source Controller](https://fluxcd.io/flux/components/source/) enables seamless integration of various Git repositories with your Kubernetes cluster. Think of the Source Controller as an interface to connect with GitRepositories, OCIRepository, HelmRepository, and Bucket resources.\\n\\n`\u2714 source-controller: deployment ready`\\n\\nThe bootstrap command will create a flux source and associate it to the repo passed in the command, to validate this you can list the git sources created and you will see the one, for now.\\n\\n```bash\\nflux get sources git\\n```\\n\\n![Flux-git-source](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kjtan9rnhp0qc9mb6l02.png)\\n\\nand you can see the K8 CDRs created\\n\\n`kubectl get crds | grep flux`\\n\\n## Structuring the Git repository\\n\\nThere are different strategies to structure the GitOps repository, for this scenario, a mono-repo strategy is used and [kustomize](https://kustomize.io/) will be used to manage the K8 manifest for the application.\\n\\n- **./clusters**: contains all the cluster associated with the project, cluster for each environment or region should be placed here.\\n\\n- **./clusters/smart-cash-develop/bootstrap:** Yaml files created by fluxcd installation, also there is a file name **core-kustomization.yaml** that points to a core folder that manages the manifests.\\n\\n- **./clusters/smart-cash-develop/core:** Contains the main manifest for the project, manifest like FluxSources, and also kustomization files. Here will be placed the kustomization file for each microservice that will be created.\\n\\n- **./clusters/smart-cash-develop/core:** Manifests that create common resources for the cluster like namespaces, ingress, storage-classes, etc.\\n\\n- **Manifests:** This contains subfolders that contain the YAML files for each microservices.\\n\\n``` txt\\n\u251c\u2500\u2500 clusters\\n    \u2514\u2500\u2500 smart-cash-develop\\n        |\u2500\u2500 bootstrap\\n        |\u2500\u2500 common\\n        |   |\u2500\u2500 ingress-namespace.yaml\\n        |   \u2514\u2500\u2500 namespaces.yaml\\n        |\u2500\u2500 core\\n        |   |\u2500\u2500 common-kustomize.yaml\\n        |   \u2514\u2500\u2500 helm-cert-manager.yaml\\n        \u2514\u2500\u2500 manifests\\n            \u2514\u2500\u2500 app1\\n                |\u2500\u2500 base\\n                |   |\u2500\u2500 kustomization.yaml\\n                |   \u2514\u2500\u2500 deployment.yaml\\n                \u2514\u2500\u2500 overlays\\n                    |\u2500\u2500 develop\\n                    |   \u2514\u2500\u2500 kustomization.yaml\\n                    \u2514\u2500\u2500 production\\n                        \u2514\u2500\u2500 kustomization.yaml \\n```\\n\\n## Adding resources to the cluster\\n\\nLet\'s create a K8 namespace to be used for an nginx-ingress. The manifest for this can be placed in the _common_ folder. A FluxCD Kustomization can be added to synchronize the contents of this folder with the K8 cluster.\\n\\nThe following is the Flux Kustomization that reconciles the Kubernetes manifests located at the path _./common_ in the Git repository .\\n\\n**Note:** This file can be added in _clusters/smart-cash-develop_ folder, FluxCD will automatically create the Kustomization resource because this path was specified in the bootstrap command, and Flux created a Kustomization to synchronize it.\\n\\n```YAML\\napiVersion: kustomize.toolkit.fluxcd.io/v1\\nkind: Kustomization\\nmetadata:\\n  name: smartcash-common\\n  namespace: flux-system\\nspec:\\n  interval: 5m\\n  targetNamespace: default\\n  sourceRef:\\n    kind: GitRepository\\n    name: flux-system\\n  path: \\"./kustomize\\"\\n  prune: true\\n```\\n\\n- **interval:** The period at which the Kustomization is reconciled.\\n- **sourceRef:** refers to the Source object with the required Artifacts, in this case, our GitOps repository.\\n- **prune:**: When is true, if previously applied objects are missing from the current revision, these objects are deleted from the cluster\\n\\nOnce you push the Yaml file to the GitOps repo, Flux will create the resources in the cluster. You can validate running:\\n\\n`kubectl get kustomization -n flux-system`\\n\\n![common-kustomization](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/s0aqez3paizw42s38imz.png)\\n\\nThe previous steps have created the FluxCD Kustomization to sync the _common_ folder with the cluster. Now, a Kustomize file needs to be added to specify which resource to create.\\n\\nDon\'t confuse the [FluxCD Kustomization](https://fluxcd.io/flux/components/kustomize/kustomizations/#path) file with the K8 configuration management [Kustomize](https://kustomize.io/). FluxCD will look for the Kustomize file in the _common_ folder.\\n\\nLet\'s create and push the following files in the _common_ folder.\\n\\n``` YAML\\napiVersion: kustomize.config.k8s.io/v1beta1\\nkind: Kustomization\\n\\nresources:\\n- ns-nginx-ingress.yaml\\n```\\n\\n```YAML\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: nginx-ingress\\n```\\n\\nYou can wait for the flux reconciliation or force it using the following command:\\n\\n```bash\\nflux reconcile kustomization smartcash-common\\n```\\n\\nIf the process was successful you should see the nginx-ingress namespace.\\n\\n### Troubleshooting\\n\\nTo validate the status of the reconciliation you can use the following command:\\n\\n``` bash\\nflux get kustomization smartcash-common\\n```\\n\\nFor instance, a mistake in the name of the YAML files caused this error, which was visible in the output of the flux command.\\n\\n![Flux-error](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2gdw0r19wyk1r285c7hz.png)\\n\\nIf you want more details you can check the K8 CDRs using:\\n\\n```bash\\nkubectl describe kustomization smartcash-common -n flux-system \\n```\\n\\n## Creating a Helm release for nginx-ingress\\n\\nThe Flux Helm Controller will be used to install the ingress. [The Helm Controller](https://fluxcd.io/flux/components/helm/) is a Kubernetes operator that enables the management of Helm chart releases.\\n\\nA FluxCD source for Helm needs to be added. This can be accomplished by using the following manifest, which should be placed in _clusters/smart-cash-develop_.\\n\\n``` YAML\\napiVersion: source.toolkit.fluxcd.io/v1beta2\\nkind: HelmRepository\\nmetadata:\\n  name: helm-repo-nginx-ingress\\n  namespace: flux-system\\nspec:\\n  interval: 5m0s\\n  type: oci\\n  url: oci://ghcr.io/nginxinc/charts\\n```\\n\\nThis source fetches the Helm OCI repository oci://ghcr.io/nginxinc/charts every 5 minutes, and the artifact is stored and updated each time new updates are done to the repository.\\n\\nAfter creating the Helm source, you can proceed to create the Helm release. This release specifies the chart to install in the cluster, with the chart being fetched from the source already created. The following manifest can be used.\\n\\n```yaml\\napiVersion: helm.toolkit.fluxcd.io/v2beta1\\nkind: HelmRelease\\nmetadata:\\n  name: nginx-ingress\\n  namespace: nginx-ingress\\nspec:\\n  interval: 10m0s\\n  chart:\\n    spec:\\n      chart: nginx-ingress\\n      version: 0.17.1\\n      sourceRef:\\n        kind: HelmRepository\\n        name: helm-repo-nginx-ingress\\n        namespace: flux-system\\n\\n```\\n\\nTo delegate the creation of the HelmRelease task to flux, this file can be added to the common folder and in the Kustomize file as well.\\n\\n``` YAML\\napiVersion: kustomize.config.k8s.io/v1beta1\\nkind: Kustomization\\n\\nresources:\\n- ns-nginx-ingress.yaml\\n- nginx-ingress-helm.yaml\\n```\\n\\nAfter updating and pushing the files, you can validate the creation of the Helm Release and nginx-ingress resources.\\n\\n``` YAML\\nflux get helmreleases -n nginx-ingress\\n```\\n\\n![Helm-releases](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ui87g1erg9j42rc1ost4.png)\\n\\nUp to this point, we\'ve covered the second phase of this project. In the upcoming articles, you\'ll delve into the implementation of various other tools and continue building the project.\\n\\nIf you have any feedback or suggestions, please feel free to reach out to me on [LinkedIn](https://www.linkedin.com/in/danielrive/)."},{"id":"github-actions","metadata":{"permalink":"/blog/github-actions","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-10-25-AWS-Infrastructure-Terraform-and-GitHub-Actions.md","source":"@site/blog/2023-10-25-AWS-Infrastructure-Terraform-and-GitHub-Actions.md","title":"AWS Infrastructure Terraform and GitHub Actions","description":"gh-actions","date":"2023-10-25T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Content related with AWS"},{"inline":true,"label":"SmartCash-Project","permalink":"/blog/tags/smart-cash-project"}],"readingTime":8.34,"hasTruncateMarker":true,"authors":[{"name":"Daniel German Rivera","title":"Cloud Engineer","url":"https://github.com/danielrive","page":{"permalink":"/blog/authors/danielrivera"},"socials":{"github":"https://github.com/danielrive","linkedin":"https://www.linkedin.com/in/danielrive/"},"key":"danielrivera"}],"frontMatter":{"slug":"github-actions","title":"AWS Infrastructure Terraform and GitHub Actions","authors":["danielrivera"],"tags":["aws","SmartCash-Project"]},"unlisted":false,"prevItem":{"title":"GitOps with FluxCD","permalink":"/blog/fluxcd"},"nextItem":{"title":"Containers - entre historia y runtimes","permalink":"/blog/containers-history"}},"content":"![gh-actions](./../static/img/gh-actions.jpg)\\n\\nThe journey to learn a new tool can be a little tricky, watching videos and reading some blogs can be an option, but watching and reading can not be enough for everyone, personally I need a hands-on approach for effective learning.\\n\\n\x3c!-- truncate --\x3e\\n\\nThat motivation drove me to embark on a personal project where I could implement the tools I had been using and those I wanted to explore. to initiate this journey I decided to create an application that helped me to follow my expenses, it could be something trivial but I needed a \\"business case\\" to begin building a solution.\\n\\nThe general idea is to have an application that helps you track the monthly expenses and filter for a particular category to see where the money is going.\\n\\nLet\'s start building the initial infrastructure needed for the application.\\n\\n[Here](https://dev.to/aws-builders/smartcash-project-gitops-with-fluxcd-3aep) you can find the GitOps implementation(Part 2 of the project).\\n\\n### Source Code\\n\\nThe version of the code used in this article can be found [here](https://github.com/danielrive/smart-cash/releases/tag/v1.0.0). only specific sections of the code are included here to provide explanations, as this approach avoids the need to paste the entire code.\\n\\n### Architecture\\n\\nThe image below shows the first version of the architecture to use, I chose Kubernetes for this project as it allows me to explore some tools for K8 and improve some skills to present a K8 certification.\\n\\n![Diagram Arch v1](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/x1b6f3mgm9voy16u0lp8.jpeg)\\n\\n### IaC\\n\\nTerraform is the tool for IaC, while I won\'t delve into a detailed terraform code explanation, I will provide some key highlights:\\n\\n#### Networking\\n\\nA [third-party](https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/latest) TF module is used. to save some aws costs the NAT gateways have been disabled.\\n\\n#### EKS\\n\\nA TF module has been developed to deploy the resources needed for an EKS cluster. I created manually an IAM user and passed it as a variable(_var.userRoleARN_) to the EKS module. This internally runs an _eksctl_ command to add it to the RBAC configs. This user will be used to operate the cluster locally.\\n\\n``` Terraform\\nresource \\"null_resource\\" \\"iam-role-cluster-access\\" {\\n  provisioner \\"local-exec\\" {\\n    command = <<EOF\\n      curl --silent --location \\"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\\" | tar xz -C /tmp\\n      /tmp/eksctl version\\n      /tmp/eksctl create iamidentitymapping --cluster ${local.eksClusterName} --region=${var.region} --arn ${var.userRoleARN} --group system:masters --username \\"AWSAdministratorAccess:{{SessionName}}\\"\\n    EOF\\n  }\\n  depends_on = [\\n    aws_eks_cluster.kube_cluster,\\n    aws_eks_node_group.worker-node-group\\n  ]\\n}\\n```\\n\\nEKS worker nodes will run in public subnets, this is because I have disabled the NAT GW to save some costs.\\n\\n## Infrastructure Pipeline\\n\\n### Branch strategy\\n\\nI will follow a common branch strategy as the following image shows. The main branch is associated with the production environment.\\n\\n![Branch-Strategy](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dxc1vvelqq1l5jjqut7d.png)\\n\\n### GitHub Actions\\n\\nGitHub actions will be used to implement the pipeline to deploy the infrastructure in AWS, if you are not familiar with GitHub actions terminology you can check the [documentation](https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions).\\n\\nYou can find the YAML files in the _.github_ folder, which also contains other 3 subfolders, let\'s explore them in detail.\\n\\n#### actions folder\\n\\nThis folder contains the [GitHub composite Actions](https://docs.github.com/en/actions/creating-actions/creating-a-composite-action) to use in the workflows, you can think of an action as a template that defines the task to execute(jobs). Now let\'s review the _terraform-plan_ action.\\n\\nThe first part defines the name and the inputs for the action, in this case, I am just defining the working directory where TF is placed as an input.\\n\\n``` Yaml\\nname: \'Terraform Plan\'\\ndescription: \'Running Terraform plan\'\\n\\ninputs:\\n  WORKING_DIRECTORY:\\n    description: \'directory where the tf code is\'\\n    required: false\\n    default: \'/infra/terraform\'\\n\\n```\\n\\nThe second part defines the tasks to execute, this action will start installing Terraform.\\n\\n``` YAML\\nruns:\\n  using: \\"composite\\"\\n  steps:\\n      - name: Terraform install\\n        id: \'install-terraform\'\\n        uses: hashicorp/setup-terraform@v2\\n        with: \\n         terraform_version: \'${{ env.TERRAFORM_VERSION }}\'\\n      - name: Validate terraform version\\n        id: validate-tf-version\\n        run: terraform version\\n        shell: bash\\n```\\n\\nNotice that the step _Terraform install_ is using an external GH action, [hashicorp/setup-terraform@v2](https://github.com/marketplace/actions/hashicorp-setup-terraform), the version of this action is specified after the @. This action is available in the [GH actions marketplace](https://github.com/marketplace?type=actions).\\n\\nThe next step is to run terraform init but I won\'t delve into detail, therefore, let\'s proceed to the final two steps.\\n\\nThe step _Run terraform plan_ runs the TF plan command passing some variables that are defined in the workflow definition, the plan generated is saved in a file named with the GH actions run id.\\n\\nFinally, the plan generated in the previous step is published as an artifact, that is used for the action created for the TF apply process.\\n\\n``` YAML\\n      - name: Run terraform plan\\n        id: terraform-plan\\n        run: | \\n            terraform plan \\\\\\n            -input=false \\\\\\n            -var \'region=${{ env.AWS_REGION }}\' \\\\\\n            -var \'environment=${{ env.ENVIRONMENT }}\' \\\\\\n            -var \'project_name=${{ env.PROJECT_NAME }}\' \\\\\\n            -out ${{ github.run_id }}.tfplan\\n        shell: bash\\n        working-directory: \'.${{ inputs.WORKING_DIRECTORY }}\'\\n\\n      - name: Publish Artifact\\n        uses: actions/upload-artifact@v3\\n        with:\\n          name: tf-plan\\n          path: \'${{ github.workspace }}${{ inputs.WORKING_DIRECTORY }}/${{ github.run_id }}.tfplan\'\\n\\n```\\n\\n#### jobs\\n\\nThis folder stores some bash scripts used in the pipelines to perform specific tasks, currently just one script is stored here, and its purpose is to create the S3 bucket and the DynamoDB tables used for the TF state.\\n\\nThe script runs some AWS CLI commands to validate if the S3 bucket and DynamoDB table exist, if not the resources are created.\\n\\nA composite action has been created to execute this script, you can find it with the name terraform-backend.\\n\\n```YAML\\nname: \'Terraform backend set-up\'\\ndescription: \'set-up terraform plan\'\\nruns:\\n  using: \\"composite\\"\\n  steps:\\n    - name: Config tf backend\\n      id: tf-backend\\n      run: ./terraform-backend.sh\\n      shell: bash\\n      working-directory: .github/workflows\\n```\\n\\n#### Workflows\\n\\nWorkflows define the triggers(commits, tags, branches...) for the pipeline and also specify the process to execute(jobs), inside the workflow you can use the composite actions already defined.\\n\\n##### Workflow Template\\n\\nI have created a workflow template that defines the common tasks for all the environments, but this does not define the triggers. let\'s take a look at the template.\\n\\n```Yaml\\nname: terraform deploy template\\non:\\n  workflow_call:\\n    inputs:\\n      AWS_REGION:\\n        description: \'aws region where the resources will be deployed\'\\n        required: true\\n        type: string\\n     secrets: \\n      AWS_ACCOUNT_NUMBER:\\n        required: true\\n```\\n\\nThe initial section defines the inputs and secrets for the workflow, the main difference between inputs and secrets is that Github hides the value of the secret in the workflow logs.\\n\\nThe second part of the template defines some common environment variables.\\n\\n```yaml\\nenv:\\n  ENVIRONMENT: ${{ inputs.ENVIRONMENT }}\\n  AWS_REGION: ${{ inputs.AWS_REGION }}\\n  PROJECT_NAME: ${{ inputs.PROJECT_NAME }}\\n  TERRAFORM_VERSION: ${{ inputs.TERRAFORM_VERSION }}\\n  AWS_IAM_ROLE_GH: \'GitHubAction-AssumeRoleWithAction\'\\n```\\n\\nYou can observe that the value for the env variable AWS_REGION is set to the value passed by the input inputs.AWS_REGION defined earlier. Why it is done? let\'s review one snippet of code from the composite action for the terraform plan to gain a better understanding.\\n\\n``` yaml\\n- name: Run terraform plan\\n        id: terraform-plan\\n        run: | \\n            terraform plan \\\\\\n            -input=false \\\\\\n            -var \'project_name=${{ env.PROJECT_NAME }}\' \\\\\\n```\\n\\nAs you can see I\'m using the env variable PROJECT_NAME and passing it as a TF variable, this is possible because in the workflow I defined the value for the variable, and this is passed down in the runner. You can use inputs here but you would need to define the same input in the composite action.\\n\\nThe last part of the template defines the jobs to execute\\n\\n```yaml\\n\\njobs:\\n## Execute bash script that  create s3 bucket and dynamodb table for Terraform backend\\n  set-up-terraform-backend:\\n    runs-on: ubuntu-latest\\n    steps:  \\n      - name: checkout-repo\\n        uses: actions/checkout@v4\\n      - name: configure aws credentials\\n        uses: aws-actions/configure-aws-credentials@v4\\n        with:\\n          role-to-assume: \'arn:aws:iam::${{ secrets.AWS_ACCOUNT_NUMBER }}:role/${{ env.AWS_IAM_ROLE_GH }}\' \\n          role-session-name: GitHub_to_AWS_via_FederatedOIDC\\n          aws-region: ${{ env.AWS_REGION }}\\n      - name: config tf backend\\n        id: tf-backend\\n        run: ./terraform-backend.sh\\n        working-directory: .github/jobs/\\n```\\n\\nThe provided code shows the job to set up the Terraform backend, this job is executed in a Ubuntu runner that is defined by runs-on.\\n\\nLet\'s check the steps executed for the job.\\n\\n1. The first step is to make a checkout of the repo into the runner, this is done by an [external composite action](https://github.com/marketplace/actions/checkout).\\n2. To execute Terraform the job needs access to AWS, this is done by an [external composite action](https://github.com/marketplace/actions/configure-aws-credentials-action-for-github-actions), to avoid pass Access and Secret keys OpenID Connect (OIDC) will be used, which allows GitHub Actions workflows to access AWS. You need to create an IAM IdP in your AWS account and associate it with an IAM role, this role must contain the permissions that Terraform needs to run properly.\\nThe details for the configuration can be checked [here](https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/configuring-openid-connect-in-amazon-web-services).\\n3. Once the AWS credentials have been configured you can call the composite action created to set up the Teraform backend.\\n\\nThe other jobs follow a similar pattern but they execute the composite actions for the Terraform plan and apply.\\n\\n##### Using the template\\n\\nOnce the template is ready you can create the workflows for each environment. let\'s review the workflow for the develop environment.\\n\\n``` yaml\\nname: Terraform infra workflow DEVELOP\\nrun-name: terraform-deploy-DEVELOP\\n\\non: \\n  push:\\n    branches:\\n      - develop\\n    paths:\\n      - \\"infra/**\\"\\n  pull_request:\\n    branches:\\n      - develop\\n\\npermissions:\\n  id-token: write # This is required for requesting the JWT\\n  contents: read  # This is required for actions/checkout\\n\\ndefaults:\\n  run:\\n    shell: bash\\n    working-directory: ./infra/terraform\\n\\njobs:\\n  ### Makes a call to the workflow template defined to execute terraform, in this case, the variables define the develop environment\\n  terraform-deploy:\\n    uses: danielrive/smart-cash/.github/workflows/run-terraform-template.yaml@develop\\n    with: \\n      AWS_REGION: \'us-west-2\'\\n      ENVIRONMENT: \'develop\'\\n      PROJECT_NAME: \'smart-cash\'\\n      TERRAFORM_VERSION: \'1.4.6\'\\n    secrets:\\n      AWS_ACCOUNT_NUMBER: ${{ secrets.AWS_ACCOUNT_NUMBER_DEVELOP }}\\n```\\n\\nI have defined two triggers for the workflow, The first trigger is when new updates are pushed to _infra_ folder within the develop branch, and the second trigger is when a new Pull request is open with the develop branch as a base.\\n\\nThe permissions section is necessary to generate a token used to establish the connection with AWS IAM IdP.\\n\\nIn the job definition, you can is where you can utilize the previously created template, you need to specify the path where the template is located and the values for the inputs defined in the template.\\n\\nYou can create other workflows for other environments and pass the respective values for inputs.\\n\\nUp to this point, you\'ve covered the first phase of this project. In the upcoming articles, you\'ll delve into the implementation of various other tools and continue building the project."},{"id":"containers-history","metadata":{"permalink":"/blog/containers-history","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-03-26-Containers-entre-historia-runtimes.md","source":"@site/blog/2023-03-26-Containers-entre-historia-runtimes.md","title":"Containers - entre historia y runtimes","description":"containers-crazy","date":"2023-03-26T00:00:00.000Z","tags":[{"inline":false,"label":"Kubernetes","permalink":"/blog/tags/kubernetes","description":"Content related with Kubernetes"}],"readingTime":8.96,"hasTruncateMarker":true,"authors":[{"name":"Daniel German Rivera","title":"Cloud Engineer","url":"https://github.com/danielrive","page":{"permalink":"/blog/authors/danielrivera"},"socials":{"github":"https://github.com/danielrive","linkedin":"https://www.linkedin.com/in/danielrive/"},"key":"danielrivera"}],"frontMatter":{"slug":"containers-history","title":"Containers - entre historia y runtimes","authors":["danielrivera"],"tags":["kubernetes"]},"unlisted":false,"prevItem":{"title":"AWS Infrastructure Terraform and GitHub Actions","permalink":"/blog/github-actions"},"nextItem":{"title":"Enabling logs and alerting in AWS EKS cluster - CloudWatch Log Insights and Metric filters","permalink":"/blog/Log-Insights"}},"content":"![containers-crazy](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/sox1n5tbcssth6p8eayl.png)\\n\\nEstudiando kubernetes gast\xe9 un tiempo considerable intentando entender muchos conceptos, por ejemplo, por todo lado se habla de _OCI compliant_, buscas _OCI_ y te lleva a _runtime-spec_, buscas _runtimes_ y te lleva a _containerd_, _runc_, _image-spec_, _cgroups_, _namespaces_, etc; puedes pasar d\xedas buscando, y mucho m\xe1s cuando eres del tipo de persona que quiere entender a fondo c\xf3mo funcionan las cosas.\\n\\n\x3c!-- truncate --\x3e\\n\\nMotivado por lo anterior, me decid\xed a escribir este post con la idea de compartir los conceptos que logr\xe9 adquirir y que me han servido para entender varias cosas del gran mundo de los containers, en algunas cosas no voy a tan bajo nivel ya que hay muchos conceptos que todav\xeda desconozco y puedo decir cosas equiviocadas.\\n\\n## Lo b\xe1sico\\n\\nIniciemos entendiendo un poco la idea detr\xe1s de los containers.\\n\\nContainers tienen como objetivo crear un ambiente virtual **_aislado_** el cual se pueda distribuir y desplegar f\xe1cilmente. Dentro del container pueden correr diferentes procesos los cuales deben estar aislados de otros corriendo en el host. El kernel de linux ofrece distintas funcionalidades que permiten la creaci\xf3n de estos ambientes. Hay dos componentes principales que quiz\xe1s son el core de todos los containers.\\n\\n### Linux namespaces\\n\\nLinux namespaces nos permite crear ambientes virtuales y aislados, estos particionan recursos del kernel y hacen que  sean visibles solo para los procesos que corren dentro del namespace, pero no para procesos externos. En otras palabras, namespaces nos facilitan el aislamiento entre procesos.\\n\\n\xbfQu\xe9 recursos se pueden particionar?, bueno esto va a depender del [tipo de namespace](https://www.redhat.com/sysadmin/7-linux-namespaces) que se este usando, por ejemplo, network namespaces nos permite encapsular los recursos relacionados con networking, como interfaces, tablas de rutas, etc. De esta forma podemos crear una red virtual dentro de nuestro namespace.\\n\\nEste [post](https://www.redhat.com/sysadmin/7-linux-namespaces) explica un poco m\xe1s en detalle los namespaces.\\n\\n### cgroups\\n\\nRecordemos que el Kernel de Linux es la interfaz principal entre el hardware y los procesos, permitiendo la comunicaci\xf3n entre estos dos y ayudando a la gesti\xf3n de recursos, por ejemplo, puede terminar procesos que consuman demasiada memoria para evitar afectar el sistema operativo. Adicionalmente pueden controlar qu\xe9 procesos pueden consumir cierta cantidad de recursos.\\n\\ncgroups es una funcionalidad del Kernel de Linux que permite organizar jer\xe1rquicamente procesos y distribuir recursos(cpu, memoria, networking, storage) dentro de dicha jerarqu\xeda.\\n\\nConfigurar cgroups puede ser un poco complejo, en mi caso estuve leyendo varios post acerca del tema y requiere cierto tiempo para entender por completo su funcionamiento. En esta [serie de posts](https://www.redhat.com/sysadmin/cgroups-part-one) creados por RedHat se habla sobe cgroups y su configuraci\xf3n a trav\xe9s de systemd, pero si se desea entrar en detalle la [documentaci\xf3n de Linux](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1/cgroups.html) puede ser de ayuda.\\n\\ncgroups y namespaces se convierten en los ingredientes secretos en la creaci\xf3n de containers, namespaces permiten aislamiento a nivel de recursos y cgroups permiten controlar los limites para dichos recursos.\\n\\nPor suerte hoy en d\xeda con una sola linea podemos crear un container, no tenemos que entrar a configurar namespaces ni cgroups.\\n\\nVeamos un poco de la evoluci\xf3n de los containers y as\xed vamos aclarando ciertas cosas.\\n\\n### Un poco de historia\\n\\nDocker fue el primero que populariz\xf3 los containers, era(o es) com\xfan asociar containers directamente con Docker, pero antes ya exist\xeda algo llamado LXC(Linux containers), el cual puede entenderse como un proveedor de ambientes virtuales en Linux que usa ciertos componentes del Kernel de Linux para crear ambientes aislados(containers).\\n\\n![Image lxc](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/jquo7g5j5iusxgy42wv3.png)\\n\\nLXC se encuentra dentro del user-space, es decir, nosotros interactuamos con LXC y este se encarga de interactuar con los componentes del kernel para permitir la creaci\xf3n de containers. Aqui un [video](https://www.youtube.com/watch?v=aIwgPKkVj8s) en donde se puede ver LXC en acci\xf3n.\\n\\n> **Nota:** Antes de LXC ya se hab\xedan desarrollado otros alternativas para la creaci\xf3n de containers como OpenVZ y Linux Vserver. LXC es mencionado inicialmente ya que es lo m\xe1s cercano a Docker que es el software con el que muchos iniciamos interactuando con containers.\\n\\n#### La llegada de Docker\\n\\nDocker empaquet\xf3 LXC en una herramienta que facilitaba m\xe1s la creaci\xf3n de containers. Al ganar popularidad se crearon mejoras y unos meses despu\xe9s Docker lanz\xf3 [libcontainer](https://github.com/opencontainers/runc/tree/main/libcontainer) el cual est\xe1 escrito en [Golang](https://github.com/opencontainers/runc/tree/main/libcontainer) y b\xe1sicamente reemplazaba LXC.\\n\\n![Docker libcontainer](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ydjpgj8o2rmm15btbqet.png)\\n\\nDocker se enfoc\xf3 m\xe1s en la creaci\xf3n de containers optimizados para el despliegue de aplicaciones mejorando la portabilidad. Este [post](https://earthly.dev/blog/lxc-vs-docker/) explica m\xe1s detalladamente las diferencias entre LXC y Docker.\\n\\n#### Definiendo un est\xe1ndar para containers\\n\\nComo alternativa a Docker, empezaron a surgir otras opciones,CoreOS por su parte lanz\xf3 [rkt(2014)](https://www.redhat.com/en/topics/containers/what-is-rkt) proponiendo mejores de seguridad, CoreOS [argumentaba](https://lwn.net/Articles/623875/) que Docker hab\xeda sido construido como un monolito el cual corr\xeda como root en el host, abriendo posibilidades a comprometer todo el host en el caso de un ataque.\\n\\nrkt usa [appc(open source container)](https://github.com/appc) con el fin de mejorar la operabilidad, appc tiene como prop\xf3sito crear un est\xe1ndar general para crear containers buscando ser vendor-independent y OS-independent.\\n\\nOtras iniciativas empezaron a surgir debido a la alta popularidad de los containers y debido a esto, en 2015 se crea [OCI(Open Container Initiative)](https://opencontainers.org/about/overview/) para definir un estandar para containers([runtimes](https://github.com/opencontainers/runtime-spec/blob/main/spec.md) e [imagenes](https://github.com/opencontainers/image-spec/blob/main/spec.md)).\\n\\n#### OCI Runtime spec\\n\\n_Runtime spec_ define la configuraci\xf3n(archivo JSON), ambiente y ciclo de vida de un container. Las configuraciones son definidas en un archivo llamado config.json, el cual contiene la metadata necesaria para la ejecuci\xf3n del container, este archivo es definido de acuerdo a plataforma a usar(windows, linux, solaris, etc).\\n\\notro concepto a destacar es el _filesystem bundle_, este es un grupo de archivos con la data y metadata para correr un container. Los principales archivos que deben contener son, el config.json mencionado anteriormente y el [rootfs(linux file system)](https://www.baeldung.com/linux/rootfs), este  _filesystem bundle_ se genera a trav\xe9s del container image.\\n\\nTodas las especificaciones para el container runtime son descritas [aqui](https://github.com/opencontainers/runtime-spec/blob/main/spec.md).\\n\\n#### OCI Image spec\\n\\nDocker en sus inicios ya hab\xeda definido las especificaciones para la creaci\xf3n de im\xe1genes[Image Manifest 2 Schema Version 2](https://docs.docker.com/registry/spec/manifest-v2-2/), al ser el m\xe1s popular, OCI parti\xf3 de este para crear un est\xe1ndar m\xe1s general, que no estuviera asociado a un vendor en espec\xedfico. _Image spec_ define como construir y empaquetar container images, personalmente no he entendido del todo el funcionamiento pero aqu\xed est\xe1 la url del [repo](https://github.com/opencontainers/image-spec) y un [blog-post](https://blog.quarkslab.com/digging-into-the-oci-image-specification.html) que contienen mayor informaci\xf3n.\\n\\nHaciendo uso del _Image spec_, se puede crear un container image que puede ser ejecutada por cualquier _OCI Runtime_, esto quiere decir que a trav\xe9s del _Image spec_ se puede generar el _filesystem bundle_, el cual es usado por el runtime para la creaci\xf3n y ejecuci\xf3n del container.\\n\\n> The Runtime Specification outlines how to run a \\"filesystem bundle\\" that is unpacked on disk. At a high-level an OCI implementation would download an OCI Image then unpack that image into an OCI Runtime filesystem bundle. At this point the OCI Runtime Bundle would be run by an OCI Runtime.\\n\\n#### Container runtimes y Kubernetes\\n\\nEn el 2015 se lanza el primer release de kubernetes, el cual usaba Docker como runtime.\\n\\nDocker decide dividir el monolito creado. libcontainer es donado a OCI y Docker empieza a trabajar en un proyecto llamado runC, este se puede ver como una herramienta que lee OCI specifications e interact\xfaa con libcontainer para la creaci\xf3n de containers. runC es independiente del Docker Engine y es donado a OCI.\\n\\nrunC es una low-level runtime por lo que tambi\xe9n se desarrolla _containerd_ el cual es como una interfaz entre el cliente y runC.\\n\\n![docker-runc-containerd](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zi3mkr4s6u5bbmj3y9ls.png \\"fuente: https://images.techhive.com/images/article/2016/04/docker-runc-100656060-large.idge.png\\")\\n\\nHasta el momento solo se ha cubierto parte del origen de los container y el origen de algunas herramientas que seguimos viendo hoy en d\xeda como runC y conteinerd. En lo que sigue del post tratar\xe9 de exponer un poco m\xe1s a fondo las _container images_ al igual que algunas _containers runtimes_.\\n\\n### Container Images\\n\\nAntes de entrar a ver las _containers runtimes_, es importante entender qu\xe9 es lo que contienen las _containers images_, para ello vamos a usar [Skopeo](https://www.redhat.com/en/topics/containers/what-is-skopeo).\\n\\nSkopeo permite manipular e inspeccionar _container images_ ya sea para Windows, Linux o MacOs. En este caso vamos a usar Skopeo para obtener \\"el contenido\\" de una imagen que se encuentra en DockerHub, esto es muy similar al comando [docker export](https://docs.docker.com/engine/reference/commandline/export/),pero en este caso no vamos a instalar Docker.\\n\\n#### copiando images con skopeo\\n\\nPara instalar skopeo se puede usar snap en ubuntu\\n\\n``` bash\\nsudo snap install skopeo --edge\\n```\\n\\nuna vez que finalice la instalaci\xf3n podemos copiar una imagen que se encuentra en DockerHub a nuestro local. En este caso se va a usar la imagen de golang.\\n\\n``` bash\\nsudo skopeo --insecure-policy copy docker://golang:latest  oci://home/ubuntu/example-dev-to/golang-image-v2\\n```\\n\\nSkopeo copia el contenido de la imagen en el destino especificado, en este caso `oci:/home/ubuntu/example-dev-to/golang-image-v2`. En la imagen se puede ver que se tiene un archivo index.json, oci-layout y un directorio llamado blobs. Esto corresponde a la estructura de archivos definidos por [OCI](https://github.com/opencontainers/image-spec/blob/main/spec.md)\\n\\n![golang-copy-skopeo](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/f76zraaa4bdgws82wr9f.png)\\n\\nel _index.json_ se puede entender como un resumen de la imagen, en donde se ve el sistema operativo y la arquitectura, adem\xe1s se especifica la ubicaci\xf3n del _image manifest_.\\n\\nEl _image manifest_ contiene metadata de la imagen al igual que las especificaciones de cada layer creada.\\n\\nRevisando el index.json vamos a encontrar lo siguiente:\\n\\n![index.json golang image](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/wxaqowqg0vbmhjvsgvsq.png)\\n\\nSe puede ver informaci\xf3n acerca del sistema operativo y arquitectura soportados por la imagen. El digest(linea 6) nos indica en que archivo se encuentra el manifest.json.\\n\\n![manifest](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7tyogxsh5q90vgolsvbu.png)\\n\\nEn el manifest(imagen anterior) se puede ver el digest para el config file y para cada una de las layers que se tienen. El [mediaType](https://github.com/opencontainers/image-spec/blob/main/media-types.md) puede entenderse como el formato de cada archivo, por ejemplo la linea 4 nos dice que el archivo config de formato json se puede identificar con el digest `bdba673e96d6e9707e2a724103e8835dbdd11dc81ad0c76c4453066ed8db29fd`. Este se puede encontrar en la carpeta blobs y va a lucir como la siguiente imagen.\\n\\n![config.json](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/74dolefjcdzij0qhcf3d.png)\\n\\nEste archivo ya contiene m\xe1s informaci\xf3n de la imagen, por ejemplo podemos ver el workdir y algunas variables de entorno.\\n\\npasemos ahora a las layers, en el manifest podemos identificar los digest para cada layers, si vemos el media type nos indica que es `v1.tar+gzip`, en este caso tenemos que descomprimir el contenido de dicho digest, para ello vamos a usar `tar`\\n\\n![unpackage-digest](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fd5iwfpygp71jutt0qcx.png)\\n\\nUna vez termine el proceso podemos analizar el resultado, en este caso vamos a tener una serie de directorios que representan el rootfs de la imagen, estos archivos van a hacer parte de un layer en espec\xedfico. Si observamos la siguente imagen podemos ver que tenemos /home, /etc y /bin, etc, los cuales representan el sistema de archivos de linux(rootfs).\\n\\n![rootfs](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/clm54sgl7p2i08gtrd1b.png)\\n\\nCon esto vemos a alto nivel el contenido de un _container image_, al final el _container runtime_ es el que se encarga de descomprimir y leer todos estos archivos, el cual va a ser usado para correr el container.\\n\\nHasta aqu\xed va la primera parte de este post, en la siguiente veremos un poco m\'as los container runtimes."},{"id":"Log-Insights","metadata":{"permalink":"/blog/Log-Insights","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-01-10-AWS-EKS-and-CloudWatch-Log-Insights.md","source":"@site/blog/2023-01-10-AWS-EKS-and-CloudWatch-Log-Insights.md","title":"Enabling logs and alerting in AWS EKS cluster - CloudWatch Log Insights and Metric filters","description":"fluent-bit","date":"2023-01-10T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Content related with AWS"},{"inline":false,"label":"Kubernetes","permalink":"/blog/tags/kubernetes","description":"Content related with Kubernetes"}],"readingTime":6.305,"hasTruncateMarker":true,"authors":[{"name":"Daniel German Rivera","title":"Cloud Engineer","url":"https://github.com/danielrive","page":{"permalink":"/blog/authors/danielrivera"},"socials":{"github":"https://github.com/danielrive","linkedin":"https://www.linkedin.com/in/danielrive/"},"key":"danielrivera"}],"frontMatter":{"slug":"Log-Insights","title":"Enabling logs and alerting in AWS EKS cluster - CloudWatch Log Insights and Metric filters","authors":["danielrivera"],"tags":["aws","kubernetes"]},"unlisted":false,"prevItem":{"title":"Containers - entre historia y runtimes","permalink":"/blog/containers-history"},"nextItem":{"title":"Enabling logs and alerting in AWS EKS cluster - CloudWatch and fluent-bit","permalink":"/blog/fluentbit-cloudwatch"}},"content":"![fluent-bit](./../static/img/fluent-bit-logo.png)\\n\\nAWS CloudWatch Logs could be used to store logs generated by resources created in AWS or external resources, once the logs are in CloudWatch you can run some queries to get specific information and create alerts for specific events.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn the [first part](https://dev.to/aws-builders/enabling-logs-and-alerting-in-eks-cluster-part-1-2gnb) of this post, I described the steps to enable logs in an EKS cluster, control plane logs, and container logs using Fluent-bit and CloudWatch, in this post I will show how to get helpful information from logs and create alerts for specific events.\\n\\n## CloudWatch Log Insights\\n\\nCloudWatch Logs Insights allows search and analysis of log data stored in Amazon CloudWatch Logs, queries can be run to identify potential causes and validate fixes, an advantage of Logs Insights is the ability to discover fields, doing more easy the process to run queries. Automatically Logs Insights define 5 fields:\\n\\n1. message: This field contains the original log message sent to CloudWatch.\\n\\n2. timestamp: contains the event timestamp registered in the original event.\\n\\n3. ingestionTime: contains the time when CloudWatch Logs received the log event.\\n\\n4. logStream: contains the name of the log stream where the event was added.\\n\\n5. log: it is an identifier in the form of account-id:log-group-name. When querying multiple log groups, this can be useful to identify which log group a particular event belongs to.\\n\\nThose fields are discovered by CloudWatch and depending on the log type that we are using CloudWatch will discover more fields, for instance, for EKS control plane logs you can see the field shown in the following image:\\n\\n![EKS logs insights fields](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qfrfyooqek9dp5mj58zl.png)\\n\\n## Running queries in AWS Log Groups\\n\\nQueries can be run to search specific events, the field discovery is really helpful in designing the query to run, in some cases when you don\'t know the structure of the logs you can run a simple query to get the fields that CloudWatch discovered, and use them to design the query based on the use case.\\n\\nAn important thing to mention here is if the CloudWatch log groups have been encrypted by KMS you must have permission to use the key.\\n\\n### How to run queries?\\n\\n1. Go to AWS CloudWatch service, in the left panel select Logs Insights.\\n\\n2. Select the logs groups to run queries, up to 20 log groups can be selected, Logs Insights will search in the groups specified.\\n\\n3. By default CloudWatch shows a simple query, you can run it and validate the fields discovered by CloudWatch. The following image shows a query that gets up to 10 results, you can check it and validate the fields.\\n\\n![logs insights panel](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pjk584k97xwc5jfcvik5.png)\\n\\n[AWS documentation](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_QuerySyntax.html) describes the query sintax that you can use.\\n\\n#### Queries examples for EKS\\n\\n##### Search API calls made by kubectl user-agent\\n\\nThe following example searches the calls made to Kube API using the kubectl command and with the GET action. In this case, the log group is the one that EKS has created when you enable logging in the cluster, in the [previous](https://dev.to/aws-builders/enabling-logs-and-alerting-in-eks-cluster-part-1-2gnb) post I mentioned the name-format and how to enable it.\\n\\n``` bash\\nfields @logStream, @timestamp, @message\\n| filter @logStream like /kube-apiserver-audit/\\n| filter userAgent like /kubectl/\\n| sort @timestamp desc\\n| filter verb like /(get)/\\n```\\n\\nThe first line is used to specify the fields that you want to show in the results, the query in the example will show the logStream name, the timestamp, and the message, you can add the fields that you want.\\n\\n![results in cloudwatch insights](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/nuvbm555wmrtljw00j16.png)\\n\\n##### Search events filtering by namespace\\n\\nYou can use the fields discovered by CloudWatch to create your queries, in this case for EKS control logs, one field discovered is objectRef.namespace, and the following query uses it to get the events where the kube-system namespace is used.\\n\\n``` bash\\nfields @timestamp, @message\\n| sort @timestamp desc\\n| filter objectRef.namespace like \'kube-system\'\\n| limit 2\\n```\\n\\nThe result of the previous query could look like:\\n\\n![results-filter-by-namespace](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uheuwtcxtarzt82q08a6.png)\\n\\n## Creating alerts for specific events\\n\\nCloudWatch logs can look for specific patterns inside the events that are sent, this allows the creation of metrics to monitor if a particular event happened and create alerts for this, for that we need to use AWS CloudWatch metric filters, this is configured directly in the Log group created and you must specify a pattern.\\n\\nTo create a metric filter you must select the Log Group to use, then in actions, you will see the option to create a metric filter.\\n\\n![enable-metric-filter](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/phprngsvpd5w70kee11i.png)\\n\\n### Defining a pattern for the filter\\n\\nWhen you are creating the filter you need to define a pattern to specify what to look for in the log file. When the logs are in JSON format is more easily define the pattern because you just need to specify the name of the key that you want to evaluate, for this case you can use the following format:\\n\\n`{ PropertySelector Operator Value }`\\n\\nFor more details about the pattern syntax you can check the [AWS documentation](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html).\\n\\nWhen the logs are not in JSON format is more tricky define the pattern , in this case you need to take in mind that each space is taken as a word in the filter, for instance, suppose that you have the following log event:\\n\\n`time=\\"2022-10-09T20:37:25Z\\" level=info msg=\\"STS response\\" accesskeyid=ABCD1234 accountid=123456789 arn=\\"arn:aws:sts::123456789:assumed-role/test\\" client=\\"127.0.0.1:1234\\" method=POST path=/authenticate`\\n\\nIn this case, you have different words separated by space, if you want to look for some word specific you need to know the exact position of the element to compare, let\'s see this with an example, in this case, i want to match the logs with a level equal to info, if you see the previous log event you can validate that leve=info is the word number 2 in the whole event, in this case, the pattern could be:\\n\\n`[word1,word2=\\"level=info\\",word3]`\\n\\nRemember that you need to include the whole word that you want to compare in this case you can use leve=info or you put the word between * which means any match with the word specified. let\'s see the result of the previous pattern.\\n\\n![Results-metrics-filter](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/21xd2b0o94kejzot1amn.png)\\n\\nIf you see, CloudWatch is showing each word defined in the pattern and the events that match.\\n\\nLet\'s see more examples to be more clear\\n\\n#### Metric Filter to alert when actions are made in AWS-AUTH configmap\\n\\nAWS-AUTH configmap is used to authenticate the user by IAM RBAC, and part of this kind of event looks like the following message:\\n\\n``` bash\\nkind:Event\\nlevel: Metadata\\nobjectRef.apiVersion: v1\\nobjectRef.name: aws-auth\\nobjectRef.namespace: kube-system\\nobjectRef.resource: configmaps\\nverb: get\\n```\\n\\nUnauthorized modifications in this configmap could be a security risk, a metric filter can be created to alert when this configmap is edited. The pattern could be:\\n\\n`{( $.objectRef.name = \\"aws-auth\\" && $.objectRef.resource = \\"configmaps\\" ) &&  ($.verb = \\"delete\\" || $.verb = \\"create\\" || $.verb = \\"patch\\"  ) }`\\n\\n#### Metric filter for 403 code response in calls to K8 API-SERVER\\n\\nThis is useful to detect several attempts to login or make calls to the cluster without valid credentials, part of this event looks like the following message:\\n\\n```bash\\nrequestURI: /\\nresponseStatus.code:403\\nresponseStatus.reason: Forbidden\\nresponseStatus.status: Failure\\nsourceIPs.0 : 12.34.56.78\\nverb: get\\n```\\n\\nThe pattern could be:\\n\\n`{$.responseStatus.code = \\"403\\" }`\\n\\n#### Metric filter to check Access Denied generated by AWS IAM-RBAC\\n\\nYou can monitor the number access-denied in API calls, this is generated by the AWS IAM-RBAC, part of this event looks like the following message:\\n\\n```bash\\nauthenticator-8c7,2022-08-08 10:04:56,\\"time=\\"\\"2020-08-04T28:43:44Z\\"\\" level=warning msg=\\"\\"access denied\\"\\" client=\\"\\"127.0.0.1:1234\\"\\" error=\\"\\"sts getCallerIdentity failed: error from AWS (expected 200, got 403)\\"\\" method=POST path=/authenticate\\"\\n```\\n\\nThe pattern could be:\\n\\n`[time,level=\\"*warning*\\",message1=\\"*access*\\",message2=\\"*denied*\\",more]`"},{"id":"fluentbit-cloudwatch","metadata":{"permalink":"/blog/fluentbit-cloudwatch","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-11-10-Enabling-logs-alerting-in-AWS-EKS-CloudWatch-and-fluent-bit.md","source":"@site/blog/2022-11-10-Enabling-logs-alerting-in-AWS-EKS-CloudWatch-and-fluent-bit.md","title":"Enabling logs and alerting in AWS EKS cluster - CloudWatch and fluent-bit","description":"robber-duck-logs","date":"2022-11-10T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Content related with AWS"},{"inline":false,"label":"Kubernetes","permalink":"/blog/tags/kubernetes","description":"Content related with Kubernetes"}],"readingTime":8.005,"hasTruncateMarker":true,"authors":[{"name":"Daniel German Rivera","title":"Cloud Engineer","url":"https://github.com/danielrive","page":{"permalink":"/blog/authors/danielrivera"},"socials":{"github":"https://github.com/danielrive","linkedin":"https://www.linkedin.com/in/danielrive/"},"key":"danielrivera"}],"frontMatter":{"slug":"fluentbit-cloudwatch","title":"Enabling logs and alerting in AWS EKS cluster - CloudWatch and fluent-bit","authors":["danielrivera"],"tags":["aws","kubernetes"]},"unlisted":false,"prevItem":{"title":"Enabling logs and alerting in AWS EKS cluster - CloudWatch Log Insights and Metric filters","permalink":"/blog/Log-Insights"},"nextItem":{"title":"AWS Event-Bridge and Lambda to copy RDS snapshots to another Region","permalink":"/blog/Event-Bridge-Lambda"}},"content":"![robber-duck-logs](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tsflwyqektvsf6vepwj9.jpg)\\n\\nIn this post I will share my experience enabling and configuring logging in an EKS cluster, creating alerts to send a notification when a specific event appears in the logs.\\n\\n\x3c!-- truncate --\x3e\\n\\nLogs are a fundamental component in our environments, these provide useful information that helps to debug in the case of any issue or to identify events that can affect the security of the application. Logs should be enabled in each component, from the infrastructure level to the application level. This brings some challenges like where to store the logs, what kind of events log, how to search events, and what to do with the logs.\\n\\nThis is the part #1 in which I show how to enable control plane logging and container logging in an EKS cluster, in [the part #2](https://dev.to/aws-builders/enabling-logs-and-alerting-in-eks-cluster-part-2-log-insights-and-metric-filters-3ped) I will show you how to enable some alerts using the logs groups created.\\n\\nBefore to start is important to mention that logs can contain private data like user information, keys, passwords, etc. For this reason, logs should be encrypted at rest and enable restrictions to access them.\\n\\n## Kubernetes Control Plane logging\\n\\nKubernetes architecture can be divided into a control plane and worker nodes, control plane contains the components that manage the cluster, components like etcd, API Server, Scheduler, and Controller Manager. Almost every action done in the cluster pass through the API Server that logs each event.\\n\\nAWS EKS manages the control plane for us, deploying and operating the necessary components. By default, EKS doesn\'t have logging enabled and actions from our side are required. Enabling EKS control plane logging is an easy task, you need to know what component log and enable it. You can enable logs for the API server, audit, authenticator, control manager, and scheduler.\\n\\nIn my opinion, audit logs and authenticator are useful because records actions done in our cluster and help us to understand the origin of the actions and requests generated by the IAM authenticator.\\n\\nBy terraform you can use the following code to create a simple cluster and enabling audit,api,authenticator, and scheduler logs.\\n\\n``` Terraform\\nresource \\"aws_eks_cluster\\" \\"kube_cluster\\" {\\n  name                      = \\"test-cluster\\"\\n  role_arn                  = aws_iam_role.role-eks.arn\\n  version                   = \\"1.22\\"\\n  enabled_cluster_log_types = [\\"audit\\", \\"api\\", \\"authenticator\\",\\"scheduler\\"]\\n  vpc_config {\\n    subnet_ids              = [\\"sub-1234\\",\\"sub-5678\\"]\\n    endpoint_private_access = true\\n    endpoint_public_access  = true\\n  }\\n}\\n```\\n\\nLogs are stored in AWS CloudWatch logs and the log group is created automatically following this name structure `/aws/eks/<cluster-name>/cluster`, inside the group you can find the log stream for each component that you enabled\\n\\n```text\\nauthenticator-123abcd\\nkube-apiserver-123abcd\\nkube-apiserver-123abcd\\n```\\n\\nBy default, the Log group created by AWS doesn\'t have encryption and retention days enabled, I recommend creating the logs group by yourself and specifying and KMS Key, and setting some time to expiry the logs, Kubernetes generates a considerable number of logs that will increase the size of the group that can impact the billing.\\n\\n## Kubernetes Containers logging\\n\\nThe steps mentioned above were to enable just logging in the control plane, to send logs generated by the applications running in the containers a log aggregator is necessary, in this case, I will use [Fluent-Bit](https://fluentbit.io/how-it-works/)\\n\\nFluent-Bit runs as a daemonSet in the cluster and sends logs to CloudWatch Logs. Fluent-Bit creates the log groups using the configuration specified in the kubernetes manifests.\\n\\nHere is important to mention that AWS has created a docker image for the daemonSet, this can be found in this [link](https://github.com/aws/aws-for-fluent-bit).\\n\\n[AWS describes the steps](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-logs-FluentBit.html) to run the daemonSet, this is done by some commands, but I will use a Kubernetes manifest that can be stored in our repository and then use Argo or Fluxcd to automate deployments.\\n\\nThe following steps show the manifests to create the objects that Kubernetes needs to send containers logs to CloudWatch, you must have access to the cluster and by kubeclt command create the resources (`kubeckt apply -f manifest-name.yml`).\\n\\n### 1. Namespace creation\\n\\nA K8 namespace is necessary, _**amazon-cloudwatch**_ name will use for this, you can change the name but make sure to use the same in the following steps.\\n\\n```yaml\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: amazon-cloudwatch\\n  labels:\\n    name: amazon-cloudwatch\\n```\\n\\n### 2. ConfigMap for aws-fluent-bit general configs\\n\\nThis configMap is necessary to specify some configurations for fluent-bit and for AWS, for instance, the cluster-name AWS use to create the logs group. In this case, I don\'t want to create an HTTP server for fluent-bit and I will read the logs from the tail, more information about this can be found [here](https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/configuration-file).\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: fluent-bit-general-configs ## you can use a different name, make sure to use the same in the following steps\\n  namespace: amazon-cloudwatch\\ndata:\\n  cluster.name: ${CLUSTERNAME}\\n  http.port: \\"\\"\\n  http.server: \\"Off\\"\\n  logs.region: ${AWS_REGION}\\n  read.head: \\"Off\\"\\n  read.tail: \\"On\\" \\n```\\n\\n### 3.Service Account, Cluster Role and Role Binding\\n\\nSome permissions are required to send logs from daemonSet to Cloudwatch, you can attach a role to the worker-nodes or use a service account with an IAM role, in this case, I will create an IAM role and associate it with a service account.\\nThe following Terraform code creates a policy and the role.\\n\\n``` Terraform\\nresource \\"aws_iam_role\\" \\"iam-role-fluent-bit\\" {\\n  name                  = \\"role-fluent-bit-test\\"\\n  force_detach_policies = true\\n  max_session_duration  = 3600\\n  path                  = \\"/\\"\\n  assume_role_policy    = jsonencode({\\n\\n{\\n    Version= \\"2012-10-17\\"\\n    Statement= [\\n      {\\n        Effect= \\"Allow\\"\\n        Principal= {\\n            Federated= \\"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/oidc.eks.${REGION}.amazonaws.com/id/${EKS_OIDCID}\\"\\n        }\\n        Action= \\"sts:AssumeRoleWithWebIdentity\\"\\n        Condition= {\\n          StringEquals= {\\n            \\"oidc.eks.${REGION}.amazonaws.com/id/${EKS_OIDCID}:aud\\": \\"sts.amazonaws.com\\",\\n\\"oidc.eks.${REGION}.amazonaws.com/id/${EKS_OIDCID}:sub\\": \\"system:serviceaccount:${AWS_CLOUDWATCH_NAMESPACE}:${EKS-SERVICE_ACCOUNT-NAME}\\"\\n          }\\n        }\\n      }\\n    ]\\n  }\\n\\n})\\n\\n}\\n\\n```\\n\\n- **EKS_OIDCID:** is the OpenID Connect for your cluster, you can get it in the cluster information or by terraform outputs.\\n- **AWS_CLOUDWATCH_NAMESPACE:** is the namespace create in the step 1, in this case amazon-cloudwatch.\\n- **ACCOUNT_ID:** is the AWS account number where the cluster was created.\\n\\nThe role needs a policy with permissions to create and put logs in cloudwatch, you can use the following code to create the policy and attach it to IAM Role created.\\n\\n``` Terraform\\n\\nresource \\"aws_iam_policy\\" \\"policy_sa_logs\\" {\\n  name        = \\"policy-sa-fluent-bit-logs\\"\\n  path        = \\"/\\"\\n  description = \\"policy for EKS Service Account fluent-bit \\"\\n  policy = <<EOF\\n{\\n    \\"Version\\": \\"2012-10-17\\",\\n    \\"Statement\\": [\\n        {\\n            \\"Effect\\": \\"Allow\\",\\n            \\"Action\\": [\\n                \\"cloudwatch:PutMetricData\\",\\n                \\"ec2:DescribeVolumes\\",\\n                \\"ec2:DescribeTags\\",\\n                \\"logs:PutLogEvents\\",\\n                \\"logs:DescribeLogStreams\\",\\n                \\"logs:DescribeLogGroups\\",\\n                \\"logs:CreateLogStream\\",\\n                \\"logs:CreateLogGroup\\",\\n                \\"logs:PutRetentionPolicy\\"\\n            ],\\n            \\"Resource\\": \\"arn:aws:logs:${REGION}:${ACCOUNT_ID}:*:*\\"\\n        }\\n    ]\\n}\\nEOF\\n}\\n\\n######## Policy attachment to IAM role ########\\n\\nresource \\"aws_iam_role_policy_attachment\\" \\"policy-attach\\" {\\n  role       = aws_iam_role.iam-role-fluent-bit.name\\n  policy_arn = aws_iam_policy.policy_sa_logs.arn\\n}\\n\\n```\\n\\nOnce the role has been created the Service account can be created, you can use the following k8 manifest for that, you should replace the IAM_ROLE variable for the ARN of the role created previously.\\n\\n```yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: fluent-bit\\n  namespace: amazon-cloudwatch\\n  annotations:\\n    eks.amazonaws.com/role-arn: \\"${IAM_ROLE}\\"\\n```\\n\\nWith the SA ready, you need to create a cluster role and associate that to the SA created, the following manifests can be used for that.\\n\\n```yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: fluent-bit-role\\nrules:\\n  - nonResourceURLs:\\n      - /metrics\\n    verbs:\\n      - get\\n  - apiGroups: [\\"\\"]\\n    resources:\\n      - namespaces\\n      - pods\\n      - pods/logs\\n      - nodes\\n      - nodes/proxy\\n    verbs: [\\"get\\", \\"list\\", \\"watch\\"]\\n\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: fluent-bit-role-binding\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: fluent-bit-role\\nsubjects:\\n  - kind: ServiceAccount\\n    name: fluent-bit\\n    namespace: amazon-cloudwatch\\n```\\n\\n### 4.ConfigMap for fluent-bit configurations\\n\\nA ConfigMap is used to specify a detailed configuration for Fluent-bit, AWS already defines a configuration, but you can add custom configs, The following [link](https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/fluent-bit/fluent-bit.yaml), shows the configurations defined by AWS, if you see, the first objects created in the YAML are the manifest defined in previous steps, in this step you just need to define the ConfigMap with name _fluent-bit-config_, I don\'t want to put here all the manifest because is a little long and can complicate the lecture of this post.\\n\\nWith this ConfigMap, Fluent Bit will create the log groups in the below table, you also have the option to create by terraform and specify encryption and retention period (i recommend this way).\\n\\n|     CloudWatch Log Group Name          | Source of the logs(Path inside the Container) |\\n|----------------------------------------|-------------------|\\n|aws/containerinsights/Cluster_Name/application                                      |  All log files in /var/log/containers                  |\\n| /aws/containerinsights/Cluster_Name/host                                       | Logs from /var/log/dmesg, /var/log/secure, and /var/log/messages                  |\\n| /aws/containerinsights/Cluster_Name/dataplane                                       | The logs in /var/log/journal for kubelet.service, kubeproxy.service, and docker.service.                  |\\n\\nIf you analyze the ConfigMap you can see the INPUTS for each source mentioned in the table.\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: fluent-bit-config\\n  namespace: amazon-cloudwatch\\n  labels:\\n    k8s-app: fluent-bit\\n\\n... OTHER CONFIGS \\n\\n### here is the INPUT configurations for application logs  \\napplication-log.conf: |\\n    [INPUT]\\n        Name                tail\\n        Tag                 application.*\\n        Exclude_Path        /var/log/containers/cloudwatch-agent*, /var/log/containers/fluent-bit*, /var/log/containers/aws-node*, /var/log/containers/kube-proxy*\\n        Path                /var/log/containers/*.log\\n\\n... OTHER CONFIGS \\n\\n    [OUTPUT]\\n        Name                cloudwatch_logs\\n        Match               application.*\\n        region              $${AWS_REGION}\\n        log_group_name      /aws/containerinsights/$${CLUSTER_NAME}/application\\n        log_stream_prefix   $${HOST_NAME}-\\n        auto_create_group   false\\n        extra_user_agent    container-insights\\n        log_retention_days  ${logs_retention_period} \\n\\n```\\n\\nThe OUTPUT in the previous manifest defines the CloudWatch log Group configuration that fluent bit will create, as you can see you can specify if the log groups should be created, the prefix for the stream, the name, and the retention period for the logs. If you are using Terraform you should set to false the option _auto\\\\_create\\\\_group_\\n\\n### 5.DaemonSet Creation\\n\\nThis is the last step, AWS also provides the manifest to create the DaemonSet, in this [link](https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/fluent-bit/fluent-bit.yaml) you can find it in the bottom of the file. As I mentioned, I don\'t want to put the whole file here, you can copy and paste the content or edit the file if you have custom configurations.\\n\\n```yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: fluent-bit\\n  namespace: amazon-cloudwatch\\n  labels:\\n    k8s-app: fluent-bit\\n... OTHER CONFIGS\\n```\\n\\nOnce you have run the above steps, you can validate that the daemonSet is running well and if everything is ok you should be the Logs groups in the AWS console with some events passed by fluent-bit DaemonSet\\n\\n### References\\n\\n- [Container-Insights-setup-logs-FluentBit](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-logs-FluentBit.html)\\n- [fluentbit official](https://docs.fluentbit.io/manual/pipeline/outputs/cloudwatch)"},{"id":"Event-Bridge-Lambda","metadata":{"permalink":"/blog/Event-Bridge-Lambda","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-03-02-Event-Bridge-Lambda-copy-RDS-snapshots.md","source":"@site/blog/2022-03-02-Event-Bridge-Lambda-copy-RDS-snapshots.md","title":"AWS Event-Bridge and Lambda to copy RDS snapshots to another Region","description":"Architecture Diagram","date":"2022-03-02T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Content related with AWS"}],"readingTime":3.67,"hasTruncateMarker":true,"authors":[{"name":"Daniel German Rivera","title":"Cloud Engineer","url":"https://github.com/danielrive","page":{"permalink":"/blog/authors/danielrivera"},"socials":{"github":"https://github.com/danielrive","linkedin":"https://www.linkedin.com/in/danielrive/"},"key":"danielrivera"}],"frontMatter":{"slug":"Event-Bridge-Lambda","title":"AWS Event-Bridge and Lambda to copy RDS snapshots to another Region","authors":["danielrivera"],"tags":["aws"]},"unlisted":false,"prevItem":{"title":"Enabling logs and alerting in AWS EKS cluster - CloudWatch and fluent-bit","permalink":"/blog/fluentbit-cloudwatch"},"nextItem":{"title":"Trusting in your IaC Terraform Compliance","permalink":"/blog/terraform-compliance"}},"content":"![Architecture Diagram](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6rjyhs8hmbwc76fkztos.png)\\n\\nA few months ago I was asked to design the DRP process(Multi-Region) for a project that used RDS(PostgreSQL). RDS instances were critical components, these stored PII information. RDS automatically takes snapshots of the instances and you can use them to recreate the instances in case of failure, these snapshots just can be used in the same region but you can share or copy them between accounts and Regions, here some [AWS Docs related RDS automatic snapshots](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html).\\n\\n\x3c!-- truncate --\x3e\\n\\nMy initial idea was to create a K8 job to run pg_dump for each RDS instance and then, upload the file to an S3 bucket created in a different region, not too bad but this requires more work to backup and restore. so I decided to copy snapshots between regions, a new challenge appeared here, how to automate that copy? In this post I will show the approach that I follow to solve this, one crucial point to mention here is that for a big number of snapshots, this solution could not be the best due to some limitations that AWS RDS to copy snapshots.\\n\\nAWS Documentation says:\\n>\\n\\"You can have up to 20 snapshot copy requests in progress to a single destination Region per account.\\"\\n\\nThis approach uses AWS event-bridge and Lambda to automate the copy process, at summary, even-bridge detects that a new RDS snapshot has been created and triggers a lambda function to copy the snapshot to the other region.\\n\\n>\\nA terraform code was created for this pods and you can check it [here](https://github.com/danielrive/blog-posts/blob/main/copy-rds-snapshots)\\n\\n## RDS Snapshot\\n\\nYou can configure automated RDS snapshots for your instance, this occurs daily during the backup window that you define in the instance creation.\\nIn this case, the automated RDS snapshot was configured in each instance, this just creates the snapshot in the same account and region where the RDS instance was created.\\n\\n## AWS Event-Bridge\\n\\nEventBridge is a serverless service that uses events to connect application components together, making it easier for you to build scalable event-driven applications. You can use it to route events from sources such as home-grown applications, AWS services, and third-party software to consumer applications across your organization.\\n\\nIn this case, AWS generates a significant number of events for some services, for RDS you can find the events divided into categories, the following table shows the events for RDS snapshots. when RDS starts an automated snapshot, AWS registers that event. You can find all the events in the [AWS documentation](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.Messages.html).\\n\\n![RDS Events](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/beogf5s4wdgk7wfbb5c3.png)\\n\\n### How to use the events?\\n\\nLet\'s start with an important concept in the EventBridge world, An event bus. This a pipeline that receives events, you can configure rules to manipulate the events and specify actions when these came. Events are represented as JSON objects and they all have a similar structure and the same top-level fields. By default, the AWS accounts have a default event bus that receives events from AWS services.\\n\\n![default event-bus](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/g2qv8j574deuvsbwz4rb.png)\\n\\nIn this case, we can create a rule using the default event-bus.\\n\\n![rds-rule-creation](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mmi4m21fibnoe4anq2k5.png)\\n\\nYou can choose the AWS service to use and AWS will show you and JSON with an example of how the event will look.\\n\\nFor our case we can use a simpler event using the EventID for automated snapshots, RDS-EVENT-0091, you can refer to the image shown at the top of the post for more information.\\n\\n``` json\\n{\\n  \\"source\\": [\\"aws.rds\\"],\\n  \\"detail-type\\": [\\"RDS DB Snapshot Event\\"],\\n  \\"account\\": [\\"1234567890\\"],\\n  \\"region\\": [\\"us-east-1\\"],\\n  \\"detail\\": {\\n     \\"SourceType\\": [\\"SNAPSHOT\\"],\\n      \\"EventID\\": [\\"RDS-EVENT-0091\\"]   \\n    }\\n}\\n```\\n\\nWith the event-pattern defined, we can specify the lambda function to execute when this event comes to the default event bus.\\n\\n![Trigger-lambda](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0l40u9o0ai1k7xehswu8.png)\\n\\nThis means that if an event is received in the default event bus and matches with the pattern specified, that will trigger the lambda and pass a JSON with the event generated, this looks like this:\\n\\n``` json\\n{\\n  \\"version\\": \\"0\\",\\n  \\"id\\": \\"844e2571-85d4-695f-b930-0153b71dcb42\\",\\n  \\"detail-type\\": \\"RDS DB Snapshot Event\\",\\n  \\"source\\": \\"aws.rds\\",\\n  \\"account\\": \\"123456789012\\",\\n  \\"time\\": \\"2018-10-06T12:26:13Z\\",\\n  \\"region\\": \\"us-east-1\\",\\n  \\"resources\\": [\\"arn:aws:rds:us-east-1:123456789012:db:mysql-instance-2018-10-06-12-24\\"],\\n  \\"detail\\": {\\n    \\"EventCategories\\": [\\"creation\\"],\\n    \\"SourceType\\": \\"SNAPSHOT\\",\\n    \\"SourceArn\\": \\"arn:aws:rds:us-east-1:123456789012:db:mysql-instance-2018-10-06-12-24\\",\\n    \\"Date\\": \\"2018-10-06T12:26:13.882Z\\",\\n    \\"SourceIdentifier\\": \\"rds:mysql-instance-2018-10-06-12-24\\",\\n    \\"Message\\": \\"Automated snapshot created\\"\\n  }\\n}\\n```\\n\\n## Lambda Function\\n\\nThe lambda function is a python code that gets the events and extracts the useful information and starts a copy in another region.\\n\\nThe lambda functions just start the copy, the function doesn\'t wait to be completed the task.\\n\\nYou can see the python code [here](https://github.com/danielrive/blog-posts/blob/main/copy-rds-snapshots/modules/python_code/copy-snapshots.py)"},{"id":"terraform-compliance","metadata":{"permalink":"/blog/terraform-compliance","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-01-30-Trusting-in-your-IaC-Terraform-Compliance.md","source":"@site/blog/2022-01-30-Trusting-in-your-IaC-Terraform-Compliance.md","title":"Trusting in your IaC Terraform Compliance","description":"fluent-bit","date":"2022-01-30T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Content related with AWS"}],"readingTime":6.785,"hasTruncateMarker":true,"authors":[{"name":"Daniel German Rivera","title":"Cloud Engineer","url":"https://github.com/danielrive","page":{"permalink":"/blog/authors/danielrivera"},"socials":{"github":"https://github.com/danielrive","linkedin":"https://www.linkedin.com/in/danielrive/"},"key":"danielrivera"}],"frontMatter":{"slug":"terraform-compliance","title":"Trusting in your IaC Terraform Compliance","authors":["danielrivera"],"tags":["aws"]},"unlisted":false,"prevItem":{"title":"AWS Event-Bridge and Lambda to copy RDS snapshots to another Region","permalink":"/blog/Event-Bridge-Lambda"},"nextItem":{"title":"Sharing secrets to ECS in an AWS multi-account architecture","permalink":"/blog/aws-secret-manager"}},"content":"![fluent-bit](./../static/img/tf-logo.png)\\n\\nInfrastructure as Code has started to be an important part of our cloud journey, giving us the ability to automate deploys and replicate our infra in multiple environments and accounts. Using code to define our infra allow us to implement some practices from developers world, like testing, version control, etc.\\n\\n\x3c!-- truncate --\x3e\\n\\n![IaC power](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8rz6ct19c0b7xr5ulfo5.jpeg)\\n\\nAs a lot of tools, IaC can introduce bugs or maybe miss configurations that then can be our headache when errors arise, or maybe audit companies come to review the security of our infra.\\n\\nIaC has enabled us to implement development practices like generating Pull Request with the new changes, this is very important when we want the revision from other coworkers, frameworks to do testing and maybe IaC tools that use common language programing help us to validate our infra-code, but this is more for technical teams and translate this to non-technical languages can be complicated, limiting the scope for the revision. Here is when terraform-compliance framework appears to help us to define rules to validate our infra and define if accomplish with the requirements defined from the technical side and business side.\\n\\nTerraform-compliance is a framework that validates the compliance in our infrastructure defined by terraform, this is based on negative testing and BDD that work together to validate our infra. This post will show how to implement this framework and add it to our DevOps pipeline used to deploy our IaC, AWS will be used as a cloud provider.\\n\\n### Requirements\\n\\nTerraform-compliance validates the code before it is deployed, we can install it using docker or via pip. In this post, we will use pip.\\n\\n- Python\\n- terraform 0.12+\\n\\n### How does this work?\\n\\nTerraform-compliance uses policies that define the features that the infrastructure must-have, like encryption enabled in the buckets, resources well tagged, etc. These policies are executed against the plan that Terraform generates and are defined using  **Behaviour Driven Development(BDD)** Principles, that use English like the language to define the policies.\\n\\nTo work with terraform we need to create a file in which the policies are defined, using BDD principles that file is named feature and contain the scenario that we want to evaluate.\\nThe structure of the file is the following:\\n\\n**Feature:** A summary about the things to validate\\n**Scenario/scenario Outline**: define the test to execute, this includes BDD directives like:\\n**GIVEN:** It is used to define a context that can be a list of resources or data that we want to check, we can see this as a filter.\\n**WHEN:** filter the context defined above, for instance, if the context defined says that will evaluate all the s3 buckets, with WHEN we can filter by tags for instance. If the condition doesn\'t pass this just skip to the next line instead of fail.\\n**THEN:** Has a similar function that WHEN but if the condition doesn\'t pass the scenario will fail.\\n**AND:** It is used to define an extra condition to our scenario, this is an optional statement.\\n\\nHere and example to evaluate if all the resources has tag defined.\\n\\n  `Scenario: Ensure all resources have tags\\n      Given I have resource that supports tags defined\\n      Then it must contain tags\\n      And its value must not be null`\\n\\nEach BDD directive has more capabilities and can be checked in [Terraform-compliance documentation](https://terraform-compliance.com/pages/bdd-references/).\\n\\nThe Feature file is executed against terraform plan, to do this terraform-compliance needs the TF plan as an input, for that we need to save our plan in an external file and pass it to terraform-compliance command that we will see later.\\n\\nWith the basic explanation made above let\'s make and example.\\n\\n### Hands on\\n\\nThis post was part of a webinar made in my current company and a Github repository was created, [here is the link](https://github.com/danielrive/epam-devops-webinar-2022).This repo contains a few AWS resources defined by terraform and uses Github actions to execute with each push the terraform-compliance framework. All the stuff that we will execute is before the terraform apply so we don\'t spend money creating resources\\n\\n![dev meme2](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/wbq72iwj2excg6dmju9a.png).\\n\\nThe project has been divided into two parts, the first one contains the terraform code to deploy the infrastructure and is located in the root of the repo, the second part is a folder named compliance that contains some files with .feature extension and defines the rules that we want to evaluate against our TF plan.\\n\\n``` hcl\\n#### Data Sources\\n\\ndata \\"aws_caller_identity\\" \\"ID_CURRENT_ACCOUNT\\" {}\\n\\n\\n###  KMS Policy \\ndata \\"aws_iam_policy_document\\" \\"kms_policy\\" {\\n  statement {\\n    sid    = \\"Enable IAM User Permissions\\"\\n    effect = \\"Allow\\"\\n    principals {\\n      type        = \\"AWS\\"\\n      identifiers = [\\"arn:aws:iam::${data.aws_caller_identity.ID_CURRENT_ACCOUNT.account_id}:root\\"]\\n    }\\n    actions = [\\n      \\"*\\"\\n    ]\\n    resources = [\\"*\\"]\\n\\n  }\\n}\\n\\n\\n##########################\\n## Secret manager\\n\\n# KMS ky to encrypt at rest secret manager\\nmodule \\"kms_secret_manager\\" {\\n  source = \\"./modules/kms\\"\\n  NAME   = \\"KMS-SecretManager-${var.environment}\\"\\n  POLICY = data.aws_iam_policy_document.kms_policy.json\\n}\\n\\nmodule \\"secret_manager\\" {\\n  source    = \\"./modules/secret_manager\\"\\n  NAME      = \\"secret_${var.environment}\\"\\n  RETENTION = 10\\n  KMS_KEY   = module.kms_secret_manager.ARN_KMS\\n}\\n\\n\\nmodule \\"secret_manager_k8\\" {\\n  source    = \\"./modules/secret_manager\\"\\n  NAME      = \\"secret_k8_${var.environment}\\"\\n  RETENTION = 10\\n  KMS_KEY   = module.kms_secret_manager.ARN_KMS\\n}\\n\\nresource \\"aws_s3_bucket\\" \\"bucket_test\\" {\\n  bucket = \\"my-bucket-forcompliance-test\\"\\n\\n  server_side_encryption_configuration {\\n    rule {\\n      /*   \\n      apply_server_side_encryption_by_default {\\n        sse_algorithm = \\"AES256\\"\\n      }\\n      */\\n\\n      apply_server_side_encryption_by_default {\\n        kms_master_key_id = module.kms_secret_manager.ARN_KMS\\n        sse_algorithm     = \\"aws:kms\\"\\n      }\\n\\n\\n    }\\n\\n  }\\n\\n  tags = {\\n    Name        = \\"bucket_${var.environment}\\"\\n    Environment = \\"develop\\"\\n    Owner       = \\"DanielR\\"\\n  }\\n}\\n\\nresource \\"aws_s3_bucket\\" \\"bucket_test2\\" {\\n  bucket = \\"my-bucket-forcompliance-test\\"\\n\\n  server_side_encryption_configuration {\\n    rule {\\n\\n      apply_server_side_encryption_by_default {\\n        kms_master_key_id = module.kms_secret_manager.ARN_KMS\\n        sse_algorithm     = \\"aws:kms\\"\\n      }\\n\\n    }\\n\\n  }\\n\\n  tags = {\\n    Name        = \\"bucket_${var.environment}\\"\\n    Environment = \\"develop\\"\\n    Owner       = \\"DanielR\\"\\n  }\\n}\\n\\nresource \\"aws_secretsmanager_secret\\" \\"secret_manager2\\" {\\n  name                    = \\"test\\"\\n  recovery_window_in_days = 10\\n  lifecycle {\\n    create_before_destroy = true\\n  }\\n  tags = {\\n    Name        = \\"test\\"\\n    Environment = \\"develop\\"\\n    Owner       = \\"DanielR\\"\\n  }\\n}\\n```\\n\\nTerraform code creates KMS key, Secret managers that are encrypted by KMS and s3 buckets with SSE with KMS.\\n\\nCompliance folder has three files and each one define a scenario, let\'s move on with the _S3.feature_ file.\\n\\n``` bash\\nFeature:  This validates if the s3 buckets has Encryption enabled using KMS\\nScenario: Ensure that s3 buckets are encrypted by KMS\\n    Given I have aws_s3_bucket resource configured\\n    When it contain server_side_encryption_configuration\\n    Then it must have apply_server_side_encryption_by_default\\n    Then it must have sse_algorithm\\n    And its value must be \\"aws:kms\\"\\n```\\n\\nWith that file TF-Compliance framework will validate if the S3 buckets created with that TF code have Server-Side-Encryption enabled but using KMS key, here a quick explanation of the file:\\n\\n- **Given** statement define the context in this case, all the resources defined by **_aws_s3_bucket_**\\n- **When** statement makes more small the context, in this case TF-compliance will check the buckets that have server_side_encryption_configuration in their definition.\\n- We have two **Then** statement and are used to defined that the buckets must to have **_apply_server_side_encryption_by_default_** and the **_sse_algorithm_** must be **_aws:kms_**.\\n\\n### Running the commands\\n\\nAs we mentioned before, we are using Github actions to run the TF-compliance command, the idea is to validate automatically if the rules are accomplished before the plan if so, the pipe will allow the TF apply command and the infra will comply with the rules defined.\\n\\nTo do that, we need to generate the terraform plan, this must be stored in a file in the same directory, then that will be an input for our TF-compliance command.\\n\\nTo do that we can run:\\n\\n``` HCL\\nTerraform init\\n\\nterraform plan -out=plan.out \\n\\n```\\n\\nplan.out is the name of the file to create, we can put any another name.\\n\\nOnce the plan is created we can go ahead with the execution of the TF-Compliance command, here is the command to do the magic\\n\\n``` HCL\\nterraform-compliance -f compliance/ -p plan.out\\n```\\n\\nWith the -f flag we specify the folder in which we are storing the rules(.features files).\\n\\nThe output will be\\n\\n![output-ok](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2wmg3xm80f9qx51r3umv.png).\\n\\nLet\'s change the code to use the default algorithm to encrypt  the s3 buckets.\\n\\n``` HCL\\nresource \\"aws_s3_bucket\\" \\"bucket_test\\" {\\n  bucket = \\"my-bucket-forcompliance-test\\"\\n\\n  server_side_encryption_configuration {\\n    rule {\\n      \\n      apply_server_side_encryption_by_default {\\n        sse_algorithm = \\"AES256\\"\\n      }\\n      \\n    }\\n\\n  }\\n\\n}\\n```\\n\\nRunning again the TF-compliance command will be:\\n\\n![output-failing](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5agyq3v0vf2keiky62x2.png).\\n\\nWith this we can validate how the framework works, we can define multiple rules and these can be checked for all the teams because are not 100% technical."},{"id":"aws-secret-manager","metadata":{"permalink":"/blog/aws-secret-manager","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2021-02-15-sharing-secrets-AWS-multi-account architecture.md","source":"@site/blog/2021-02-15-sharing-secrets-AWS-multi-account architecture.md","title":"Sharing secrets to ECS in an AWS multi-account architecture","description":"Alt Text","date":"2021-02-15T00:00:00.000Z","tags":[{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"Content related with AWS"}],"readingTime":5.14,"hasTruncateMarker":true,"authors":[{"name":"Daniel German Rivera","title":"Cloud Engineer","url":"https://github.com/danielrive","page":{"permalink":"/blog/authors/danielrivera"},"socials":{"github":"https://github.com/danielrive","linkedin":"https://www.linkedin.com/in/danielrive/"},"key":"danielrivera"}],"frontMatter":{"slug":"aws-secret-manager","title":"Sharing secrets to ECS in an AWS multi-account architecture","authors":["danielrivera"],"tags":["aws"]},"unlisted":false,"prevItem":{"title":"Trusting in your IaC Terraform Compliance","permalink":"/blog/terraform-compliance"}},"content":"![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/hc1ewvgl19l0b0yeqser.png)\\n\\nIn this post, We\'ll see the highlight steps on how to share secrets between AWS account using  ECS Fargate that get the secrets stored in AWS Secret manager but in a different account. We\'ll use Terraform to build this.\\n\\n\x3c!-- truncate --\x3e\\n\\nWhen We are developing our application the best practice to pass secret inside our code is to use environment variables, Secret Manager is a great option to store our variables, We can rotate them and has direct integration with some AWS services like RDS and Redshift.\\n\\nOn another side AWS Multi-account architecture can be a good election if We want to separate some resources and maintain isolated environments, AWS provides us different ways to achieve that, being AWS control tower a good way to create and government multi-account architectures. The following image shows an AWS multi-account architecture with three accounts, Production, Develop, and Security, the last one can be used to store our environment variables, and there We can define strong policies to limit the access to the resources.\\n\\nPrevious image shows the architecture that We\'ll use, you can find the terraform code in this [Link](https://github.com/danielrive/aws-ecs-iac). A Secret Manager will be created in the Security account, this secret is encrypted using a custom KMS Key, We can\'t use the default KMS key because that is managed by AWS, and We can not control it and modify its policy. ECS task will use an IAM role with the correct permissions to get secrets from the Security account and put them into the container.\\n\\n## Terraform Code\\n\\nThe Terraform code is using Modules stored in the same repo, and use AWS profiles(.aws/credentials) to get the credentials to create resources, in this case, We have two profiles, the first one for a Develop Account and the second one for a Security account. To specify the profiles you must set the names in two terraform variables.\\n\\n``` terraform\\nprovider \\"aws\\" {\\n  profile = var.PROFILE_NAME1 # profile name for dev account\\n  region  = var.REGION\\n}\\n\\nprovider \\"aws\\" {\\n  profile = var.PROFILE_NAME2\\n  region  = var.REGION\\n  alias   = \\"Security_Account\\"\\n}\\n```\\n\\n## Steps\\n\\n**1. Create IAM Roles - Develop Account**\\nThe first resource that We need to create is the IAM Role that the containers will use, this role must have permissions to get secrets and use the KMS to decrypt the secret. This role must be created in the account in which you\'ll create the ECS resources. Go to [GitHub](https://github.com/danielrive/aws-ecs-iac) repo to see in detail the Modules.\\n\\n``` terraform\\n## Module to create an IAM ROLE for ECS Task\\nmodule \\"ECS_ROLE\\" {\\n  source          = \\"./Modules/IAM\\"\\n  CREATE_ECS_ROLE = true\\n  NAME            = \\"ECS-Role-TASK\\"\\n}\\n```\\n\\n**2. Create KMS key - - Security Account**\\nOnce you have the ECS role and its ARN you can create the KMS key, this key must be created in the security account and the policy should allow to ECS role created in Step 1 to describe and decrypt the key.\\n\\n``` terraform\\n# KMS POLICY for the Key\\ndata \\"aws_iam_policy_document\\" \\"KMS_POLICY\\" {\\n  statement {\\n    sid    = \\"AllowUseOfTheKey\\"\\n    effect = \\"Allow\\"\\n    principals {\\n      type        = \\"AWS\\"\\n      identifiers = [module.ECS_ROLE.ARN_ROLE]\\n    }\\n    actions = [\\n      \\"kms:Decrypt\\",\\n      \\"kms:DescribeKey\\",\\n    ]\\n    resources = [\\"*\\"]\\n  }\\n}\\n```\\n\\nHow this resource must be created in the Security account We need to specify the provider when We call the Terraform Module.\\n\\n```terraform\\n### Module to create a KMS Key\\n\\nmodule \\"KMS_SECRET_MANAGER\\" {\\n  source = \\"./Modules/KMS\\"\\n  NAME   = \\"KMS-key-SecretManager\\"\\n  providers = {\\n    aws = aws.Security_Account   # alias for security account defined in the first lines of this code\\n  }\\n  POLICY = data.aws_iam_policy_document.KMS_POLICY.json\\n}\\n```\\n\\n**3. Create Secret Manager - Security Account**\\nWith the KMS Key ready We can continue with the Secret Manager creation, for this case, We\'ll attach to the secret manager a resource-based policy to allow access to ECS role created in the Develop account.\\n\\n```terraform\\n### Resource based policy for Secret Manager\\ndata \\"aws_iam_policy_document\\" \\"SECRET_MANAGER_POLICY\\" {\\n  statement {\\n    sid    = \\"AllowUseSecrerManager\\"\\n    effect = \\"Allow\\"\\n    actions = [\\n      \\"secretsmanager:*\\"\\n    ]\\n    principals {\\n      type        = \\"AWS\\"\\n      identifiers = [module.ECS_ROLE.ARN_ROLE]\\n    }\\n    resources = [\\"*\\"]\\n  }\\n}\\n```\\n\\n```terraform\\n## Module to create a Secret Manager with a resource based policy\\nmodule \\"SECRET_MANAGER\\" {\\n  source    = \\"./Modules/SecretManager\\"\\n  NAME      = \\"SECRET_MANAGER_TEST1\\"\\n  RETENTION = 10\\n  KMS_KEY   = module.KMS_SECRET_MANAGER.ARN_KMS\\n  POLICY    = data.aws_iam_policy_document.SECRET_MANAGER_POLICY.json\\n}\\n```\\n\\n**4. Create ECS resources and ALB - Develop Account**\\nThe last step is to create the ECS resources and the ALB.\\n\\n```terraform\\n### Module to create a Task Definition\\nmodule \\"ECS_TASK_DEFINITION\\" {\\n  depends_on     = [module.SECRET_MANAGER]\\n  source         = \\"./Modules/ECS/TaskDefinition\\"\\n  NAME           = \\"test\\"\\n  ARN_ROLE       = module.ECS_ROLE.ARN_ROLE\\n  CPU            = 512\\n  MEMORY         = \\"1024\\"\\n  DOCKER_REPO    = \\"alpine\\"\\n  REGION         = \\"us-west-2\\"\\n  SECRET_ARN     = module.SECRET_MANAGER.SECRET_ARN\\n  CONTAINER_PORT = 80\\n}\\n### Module to create a Target Group\\nmodule \\"TARGET_GROUP\\" {\\n  source              = \\"./Modules/ALB\\"\\n  CREATE_TARGET_GROUP = true\\n  NAME                = \\"testing\\"\\n  PORT                = 80\\n  PROTOCOL            = \\"HTTP\\"\\n  VPC                 = \\"vpc-0dfa4368a6f7bf90d\\"\\n  TG_TYPE             = \\"ip\\"\\n  HEALTH_CHECK_PATH   = \\"/\\"\\n  HEALTH_CHECK_PORT   = 80\\n}\\n```\\n\\nThe Task Definition is responsible to define the secret manager to use. If We go through terraform code for Task Definition We can see that in the container definition section We specify the secret manager ARN.\\n\\n``` terraform\\n\\"secrets\\": [\\n            {\\n             \\"name\\" : \\"VARIABLE_TESTING\\",\\n             \\"valueFrom\\" :  ${var.SECRET_ARN}\\n            }\\n          ]\\n```\\n\\nWe need to keep in mind that ECS will get the full secrets stored in the secret manager(key/value) into the variable that you specify, in this case, *VARIABLE_TESTING*(feel free to change the name), which means that the *VARIABLE_TESTING* will contain the full secrets in plaintext.\\n\\nfor instance, suppose We have three secrets in Secret Manager\\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/3qsw767bqbpfap56k93y.png)\\n\\nin plaintext\\n\\n``` terraform\\n{\\n  \\"endpoint\\": \\"www.testingecs.com\\",\\n  \\"pass\\": \\"1234567\\",\\n  \\"token\\": \\"34asda3asdas4q34\\"\\n}\\n```\\n\\nAccording to the above mentioned the variable *VARIABLE_TESTING* will contain the plaintext representation and if We run `echo $VARIABLE_TESTING` inside the container We will get the plaintext representation.\\n\\nTo extract the secrets from *VARIABLE_TESTING* We can implement a bash script and run it as Entrypoint.\\n\\nWith the steps showed above We can get secrets stored in another AWS account, which helps us to limit the access and create a special policy for the security account with the permissions necessary for users and AWS services.\\n\\n### References\\n\\n* [AWS documentation 1](https://aws.amazon.com/es/premiumsupport/knowledge-center/secrets-manager-share-between-accounts/)\\n\\n* [AWS documentation 2](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html)"}]}}')}}]);